{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DviY1i0i0gzq"
      },
      "source": [
        " # Boltz-2 NIM: Protein‚ÄìLigand Co-Folding and Affinity Prediction\n",
        "\n",
        " **Boltz-2 NIM** is designed to predict protein-ligand co-folding secondary structures and binding affinities. This demo also includes **MSA-Search NIM** for improved sequence alignment, protein folding, and enhanced ligand‚Äìprotein interaction prediction.\n",
        "\n",
        " ## Demo\n",
        "\n",
        " - **Platform**: Google Colab\n",
        " - **Date**: 05 Sept 2025\n",
        "\n",
        " ## Case Study: RORc Nuclear Receptor Antagonists\n",
        "\n",
        " This demo uses structures (SMILES) and pIC‚ÇÖ‚ÇÄ values from the following publications:\n",
        "\n",
        " - BMCL 2014, p.2182 https://www.sciencedirect.com/science60894X14002546\n",
        " - BMCL 2014, p.3891 https://www.sciencedirect.com/science/article006763\n",
        " - JMC 2015, p.5308 https://pubs.acs.org/doi/10.1021/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clEIwbFS0gzs"
      },
      "source": [
        " ### NVIDIA NIMs (NVIDIA Inference Microservices)\n",
        "\n",
        " [MSA-Search](https://docs.nvidia.com/nim/bionemo/msa-search/latest/overview.html) Multiple Sequence Alignment (MSA) compares a query amino acid sequence to protein databases, aligning similar sequences to identify conserved regions despite differences in length or motifs. The resulting alignments enhance structural prediction models like AlphaFold2 and OpenFold by leveraging the structural similarity of homologous sequences.\n",
        "\n",
        " [Boltz-2](https://docs.nvidia.com/nim/bionemo/boltz2/latest/index.html) Boltz-2 NIM delivers advanced biomolecular structure and binding affinity predictions for proteins, RNA, DNA, and other molecules. Built on the Boltz-2 architecture, it enables accurate modeling of complex structures and quantifies molecular interactions across diverse configurations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tykWTdQc0gzs"
      },
      "source": [
        "## 1.1 Setup the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6I54RA7S0gzs",
        "outputId": "008f5ed1-9c08-4c10-d59c-d2e84c6124c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (0.28.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx) (4.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib numpy pandas scikit-learn tqdm httpx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wUdm_iDR0gzt"
      },
      "outputs": [],
      "source": [
        "import os, requests, re, sys, shutil, logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Optional, Tuple, List\n",
        "from time import perf_counter, sleep\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from google.colab import userdata, files\n",
        "\n",
        "# Check for required dependencies\n",
        "missing_deps = []\n",
        "try:\n",
        "    import httpx\n",
        "except ImportError:\n",
        "    missing_deps.append(\"httpx\")\n",
        "\n",
        "if missing_deps:\n",
        "    print(\"Error: Missing required dependencies. Please install them using:\")\n",
        "    print(f\"pip install {' '.join(missing_deps)}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GGW_FaI0gzt"
      },
      "source": [
        "### Define input file with SMILES data and output directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0kCseY900gzt"
      },
      "outputs": [],
      "source": [
        "# edit to match your dataset path. Ensure you use the `raw` URL path if the file is hosted on GitHub.\n",
        "\n",
        "CSV_FILE = \"https://raw.githubusercontent.com/bf-nv/bionemo_tutorials/refs/heads/main/RORc_SMILES_and_pIC50.csv\"\n",
        "# CSV_FILE = \"RORc_SMILES_and_pIC50.csv\"\n",
        "\n",
        "OUTPUT_DIR = \"/content/output\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-EqQb3d0gzt"
      },
      "source": [
        "## 1.2 Setup `output` directory and `API_KEY`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2q3g35Nx0gzu"
      },
      "outputs": [],
      "source": [
        "API_KEY = userdata.get('API_KEY')\n",
        "\n",
        "# Ensure output directory exists, create if not present\n",
        "if os.path.exists(OUTPUT_DIR):\n",
        "    shutil.rmtree(OUTPUT_DIR)\n",
        "    os.makedirs(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbkfdLeg0gzu"
      },
      "source": [
        "## 1.3 Define `MSA-Search` functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "s-ttYP9c0gzu"
      },
      "outputs": [],
      "source": [
        "MSA_DATABASES = ['Uniref30_2302', 'colabfold_envdb_202108', 'PDB70_220313']\n",
        "\n",
        "def msa_search(sequence, API_KEY, databases=MSA_DATABASES):\n",
        "    msa_search_url = \"https://health.api.nvidia.com/v1/biology/colabfold/msa-search/predict\"\n",
        "    payload = {\n",
        "        \"sequence\": sequence,\n",
        "        \"databases\": databases,\n",
        "        \"e_value\": 0.0001,\n",
        "        \"iterations\": 1,\n",
        "        \"max_msa_sequences\": 10000,\n",
        "        \"run_structural_template_search\": False,\n",
        "        \"output_alignment_formats\": [\"a3m\"],\n",
        "    }\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        \"content-type\": \"application/json\",\n",
        "        \"NVCF-POLL-SECONDS\": \"300\",\n",
        "    }\n",
        "    # Call MSA-Search NIM\n",
        "    response = requests.post(msa_search_url, json=payload, headers=headers)\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "def parse_sequences(input_string, n, sequence):\n",
        "    \"\"\"\n",
        "    Parse the output of alignments from the MSA-Search NIM to be used downstream\n",
        "\n",
        "    Args:\n",
        "        input_string (str): The output file of alignments in a string format\n",
        "        n (int): The amount of alignments to return from the output when parsing\n",
        "        sequence (str): The query sequence for alignment\n",
        "\n",
        "    Returns:\n",
        "        list: A list of alignment identifiers and sequences, starting with the query,\n",
        "              where the amount of sequences is given by n\n",
        "    \"\"\"\n",
        "    # Output is parsed to have a line for the sequence id and sequence itself so `n` returns correlates to n*2 lines\n",
        "    n = n * 2\n",
        "    # Regex to find blocks starting with `>` and then followed by a sequence.\n",
        "    pattern = re.compile(r'\\n>(.*?)\\n(.*?)(?=\\n>|\\Z)', re.DOTALL)\n",
        "    matches = pattern.finditer(input_string)\n",
        "    output_list_to_order = []\n",
        "    for match in matches:\n",
        "        # The name is the first capturing group, split by tab and take the first part\n",
        "        name_full = match.group(1).split('\\t')[0]\n",
        "        SW_score = match.group(1).split('\\t')[1]\n",
        "        # The sequence is the second capturing group\n",
        "        sequence_raw = match.group(2).strip()\n",
        "        aligned_sequence = ''.join(char for char in sequence_raw if char.isupper() or char == '-')\n",
        "        # Store the aligned sequence in the list of outputs by name, sequence, Smith-Waterman score\n",
        "        output_list_to_order.append((f'>{name_full}', aligned_sequence, int(SW_score)))\n",
        "    output_lines = output_list_to_order[:n]\n",
        "    return output_lines\n",
        "\n",
        "\n",
        "def validate_a3m_format(alignments_string):\n",
        "    \"\"\"\n",
        "    Validate that the alignment string follows A3M format.\n",
        "\n",
        "    Args:\n",
        "        alignments_string (str): String containing alignments\n",
        "\n",
        "    Returns:\n",
        "        bool: True if valid A3M format, False otherwise\n",
        "    \"\"\"\n",
        "    lines = alignments_string.strip().split('\\n')\n",
        "    if len(lines) < 2:\n",
        "        return False\n",
        "\n",
        "    # Check that we have alternating header and sequence lines\n",
        "    valid_aa_chars = set('ACDEFGHIKLMNPQRSTVWY-')\n",
        "    for i, line in enumerate(lines):\n",
        "        if i % 2 == 0:  # Even indices should be headers\n",
        "            if not line.startswith('>'):\n",
        "                return False\n",
        "        else:  # Odd indices should be sequences\n",
        "            if line.startswith('>'):\n",
        "                return False\n",
        "            # Sequences should only contain valid amino acid characters and gaps\n",
        "            if not all(c in valid_aa_chars for c in line.upper()):\n",
        "                return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def write_alignments_to_a3m(alignments_data, uniprot_id, output_dir):\n",
        "    \"\"\"\n",
        "    Write alignment data to a3M format file.\n",
        "\n",
        "    Args:\n",
        "        alignments_data: Either a list of alternating headers/sequences or a string containing alignments\n",
        "        uniprot_id (str): Uniprot ID of the protein\n",
        "        output_dir (str): Directory for the output a3M file\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the created a3M file\n",
        "    \"\"\"\n",
        "    # Ensure output directory exists\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    output_path = Path(output_dir) / f\"{uniprot_id}_msa_alignments.a3m\"\n",
        "\n",
        "    # Handle both list and string input formats\n",
        "    if isinstance(alignments_data, list):\n",
        "        alignments_string = '\\n'.join(alignments_data)\n",
        "    elif isinstance(alignments_data, str):\n",
        "        alignments_string = alignments_data\n",
        "    else:\n",
        "        raise ValueError(\"alignments_data must be either a list or string\")\n",
        "\n",
        "    # Validate A3M format\n",
        "    if not alignments_string.strip():\n",
        "        raise ValueError(\"Empty alignment data provided\")\n",
        "\n",
        "    # Count sequences for reporting\n",
        "    sequence_count = alignments_string.count('>')\n",
        "    if sequence_count == 0:\n",
        "        raise ValueError(\"No sequences found in alignment data\")\n",
        "\n",
        "    # Validate A3M format structure\n",
        "    if not validate_a3m_format(alignments_string):\n",
        "        print(\"Warning: Alignment data may not follow strict A3M format\")\n",
        "        print(\"Proceeding with file creation...\")\n",
        "\n",
        "    print(f\"Writing {sequence_count} sequences to A3M format: {output_path}\")\n",
        "\n",
        "    try:\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            # Write the alignments\n",
        "            f.write(alignments_string)\n",
        "            # Ensure file ends with newline\n",
        "            if not alignments_string.endswith('\\n'):\n",
        "                f.write('\\n')\n",
        "\n",
        "        # Verify the file was created successfully\n",
        "        if output_path.exists():\n",
        "            file_size = output_path.stat().st_size\n",
        "            print(f\"Successfully created A3M file:\")\n",
        "            print(f\"File: {output_path}\")\n",
        "            print(f\"Size: {file_size:,} bytes\")\n",
        "            print(f\"Sequences: {sequence_count}\")\n",
        "\n",
        "            # Download the file to the user's machine\n",
        "            try:\n",
        "                files.download(str(output_path))\n",
        "                print(f\"File downloaded successfully: {output_path}\")\n",
        "            except Exception as download_error:\n",
        "                print(f\"Warning: Could not download file automatically: {download_error}\")\n",
        "                print(f\"File is available at: {output_path}\")\n",
        "\n",
        "            return str(output_path)\n",
        "        else:\n",
        "            raise IOError(f\"Failed to create file {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing A3M file: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def process_msa_alignments(msa_response_dict, sequence, uniprot_id, output_dir, databases=MSA_DATABASES, max_sequences_per_db=10000):\n",
        "    \"\"\"\n",
        "    Process MSA alignments from multiple databases and merge them into A3M format.\n",
        "\n",
        "    Args:\n",
        "        msa_response_dict (dict): MSA response data containing alignments\n",
        "        sequence (str): Query sequence for alignment\n",
        "        uniprot_id (str): Uniprot ID of the protein\n",
        "        output_dir (str): Output directory for the A3M file\n",
        "        databases (list): List of database names to process\n",
        "        max_sequences_per_db (int): Maximum number of sequences to parse per database\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the created A3M file\n",
        "    \"\"\"\n",
        "    all_parsed_dataset_output = []\n",
        "    for database in databases:\n",
        "        print(f\"Parsing results from database: {database}\")\n",
        "        # Pull string of alignments stored in json output for specific dataset\n",
        "        alignment_data = msa_response_dict['alignments'][database]['a3m']['alignment']\n",
        "        parsed_alignments = parse_sequences(alignment_data, max_sequences_per_db, sequence)\n",
        "        print(f\"Number of sequences aligned: {len(parsed_alignments)}\")\n",
        "        all_parsed_dataset_output.extend(parsed_alignments)\n",
        "    # Sort all the alignments based off of the alignment score\n",
        "    all_parsed_dataset_output.sort(key=lambda x: x[2], reverse=True)\n",
        "    # Now that the alignments across all datasets are sorted, reformat each entry to name and sequence\n",
        "    sorted_parsed_output_formatted = []\n",
        "    for align_tuple in all_parsed_dataset_output:\n",
        "        sorted_parsed_output_formatted.extend([align_tuple[0], align_tuple[1]])\n",
        "    merged_alignments_protein = [f\">query_sequence\\n{sequence}\"] + sorted_parsed_output_formatted\n",
        "    print(f\"Total merged alignments: {len(merged_alignments_protein)}\")\n",
        "    # Write merged_alignments_protein to a3M format\n",
        "    a3m_file_path = write_alignments_to_a3m(\n",
        "        merged_alignments_protein,\n",
        "        uniprot_id,\n",
        "        output_dir\n",
        "    )\n",
        "    return a3m_file_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_gSU6ss0gzu"
      },
      "source": [
        "## 1.4 Run `MSA-Search` and save `A3M` alignment file output to local directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_LK4WpK0gzv"
      },
      "source": [
        "### Provide sequence information\n",
        "**NOTE:** Ensure the sequence is a string and does not contain any whitespace, special characters, nor carriage returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CEsnaqNa0gzv"
      },
      "outputs": [],
      "source": [
        "# Example sequence using human RORc from PDB:4wqp\n",
        "# http://rcsb.org/structure/4WQP\n",
        "\n",
        "# >4WQP_1|Chain A|Nuclear receptor ROR-gamma|Homo sapiens (9606)\n",
        "uniprot_id = \"4wqp_1\"\n",
        "sequence = \"MHHHHHHGENLYFQGSAPYASLTEIEHLVQSVCKSYRETCQLRLEDLLRQRSNIFSREEVTGYQRKSMWEMWERCAHHLTEAIQYVVEFAKRLSGFMELCQNDQIVLLKAGAMEVVLVRMCRAYNADNRTVFFEGKYGGMELFRALGCSELISSIFDFSHSLSALHFSEDEIALYTALVLINAHRPGLQEKRKVEQLQYNLELAFHHHLCKTHRQSILAKLPPKGKLRSLCSQHVERLQIFQHLHPIVVQAAFPPLYKELFSGNS\"\n",
        "sequences = [(uniprot_id, sequence)]\n",
        "\n",
        "# NOTE: Ensure the sequence is a string and does not contain any whitespace, special characters, nor carriage returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ib540vPP0gzv",
        "outputId": "21297613-5deb-4e4c-d628-f7094f08ab4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing protein: 4wqp_1\n",
            "Sequence length: 265\n",
            "Parsing results from database: Uniref30_2302\n",
            "Number of sequences aligned: 100\n",
            "Parsing results from database: colabfold_envdb_202108\n",
            "Number of sequences aligned: 100\n",
            "Parsing results from database: PDB70_220313\n",
            "Number of sequences aligned: 88\n",
            "Total merged alignments: 577\n",
            "Warning: Alignment data may not follow strict A3M format\n",
            "Proceeding with file creation...\n",
            "Writing 289 sequences to A3M format: /content/output/4wqp_1_msa_alignments.a3m\n",
            "Successfully created A3M file:\n",
            "File: /content/output/4wqp_1_msa_alignments.a3m\n",
            "Size: 81,161 bytes\n",
            "Sequences: 289\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c4b51b6d-e041-4742-a5a1-cb961dc3fa58\", \"4wqp_1_msa_alignments.a3m\", 81161)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:38<00:00, 38.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully: /content/output/4wqp_1_msa_alignments.a3m\n",
            "Successfully processed 4wqp_1 -> /content/output/4wqp_1_msa_alignments.a3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for seq_id, seq in tqdm(sequences):\n",
        "    try:\n",
        "        print(f\"\\nProcessing protein: {seq_id}\")\n",
        "        print(f\"Sequence length: {len(seq)}\")\n",
        "\n",
        "        # Call MSA-Search NIM\n",
        "        msa_response_dict = msa_search(seq, API_KEY)\n",
        "\n",
        "        # Check if the response contains the expected data\n",
        "        if 'alignments' not in msa_response_dict:\n",
        "            print(f\"Warning: No alignments found for {seq_id}\")\n",
        "            continue\n",
        "\n",
        "        # Process and create A3M file\n",
        "        a3m_file_path = process_msa_alignments(msa_response_dict, seq, seq_id, OUTPUT_DIR)\n",
        "        print(f\"Successfully processed {seq_id} -> {a3m_file_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {seq_id}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F_wZFKG0gzv"
      },
      "source": [
        "## 1.5 List all created `A3M` alignment files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QxPhu4XP0gzv",
        "outputId": "24de18aa-fbd7-4d69-e0d4-b7708b2d46d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 1 A3M files:\n",
            "  - 4wqp_1_msa_alignments.a3m (81,161 bytes)\n",
            "\n",
            "All A3M files are available in: /content/output\n",
            "Files have been automatically downloaded to your machine.\n"
          ]
        }
      ],
      "source": [
        "# List all created A3M files\n",
        "a3m_files = list(Path(OUTPUT_DIR).glob(\"*.a3m\"))\n",
        "a3m_files = sorted(a3m_files)\n",
        "print(f\"Created {len(a3m_files)} A3M files:\")\n",
        "for file_path in a3m_files:\n",
        "    file_size = Path(file_path).stat().st_size\n",
        "    print(f\"  - {Path(file_path).name} ({file_size:,} bytes)\")\n",
        "\n",
        "print(f\"\\nAll A3M files are available in: {OUTPUT_DIR}\")\n",
        "print(\"Files have been automatically downloaded to your machine.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO5Zt1Us0gzv"
      },
      "source": [
        "## 1.6 If needed, trigger download of all `A3M` alignment files to local machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LVzQZ_oP0gzv",
        "outputId": "fc66c11d-ff18-487c-c903-d171fc078716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2c80e752-ec73-41f0-ab21-97bcfb37bbcc\", \"4wqp_1_msa_alignments.a3m\", 81161)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "for file_path in a3m_files:\n",
        "    files.download(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL35ZbHV0gzv"
      },
      "source": [
        "## 2.1 Set-up `Boltz-2` environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUszs6ES0gzv"
      },
      "source": [
        "### Configuration constraints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hDto6kLs0gzv"
      },
      "outputs": [],
      "source": [
        "# MSA configuration\n",
        "MSA_FILE_PATH = Path(OUTPUT_DIR) / f\"{uniprot_id}_msa_alignments.a3m\"\n",
        "MSA_STATUS = MSA_FILE_PATH.exists()\n",
        "\n",
        "# Sequence configuration\n",
        "SEQUENCE_ID = uniprot_id\n",
        "SEQUENCE_STRING = sequence\n",
        "\n",
        "# Boltz2 parameters\n",
        "STATUS_URL = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/status/{task_id}\"\n",
        "BOLTZ2_ENDPOINT = \"https://health.api.nvidia.com/v1/biology/mit/boltz2/predict\"\n",
        "NVCF_POLL_SECONDS = 300\n",
        "MANUAL_TIMEOUT_SECONDS = 400\n",
        "\n",
        "BOLTZ2_CONFIG = {\n",
        "    \"recycling_steps\": 3,\n",
        "    \"sampling_steps\": 20,\n",
        "    \"diffusion_samples\": 1,\n",
        "    \"step_scale\": 1.64,\n",
        "    \"without_potentials\": True\n",
        "}\n",
        "\n",
        "# Generate timestamp for output files\n",
        "timestamp = datetime.now().strftime(\"%Y_%m_%d\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlSkZ4rj0gzv"
      },
      "source": [
        "## 2.2 Use `CSV_FILE` as source of SMILES and ground-truth pIC50 data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sm55RtEm0gzw",
        "outputId": "8a851d4d-dca7-43d2-c4b8-748c5b87803d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded dataset: (22, 3)\n",
            "Columns: ['smiles', 'actual_pIC50', 'reference']\n",
            "\n",
            "First few rows:\n",
            "                                              smiles  actual_pIC50  \\\n",
            "0  CC(=O)N3CCN(c2ccc(CC(CC(C)C)S(=O)(=O)Cc1ccccc1...          7.43   \n",
            "1  CC(=O)N4CCN(c3ccc(CC(C1CCC1)S(=O)(=O)Cc2ccccc2...          7.96   \n",
            "2  CC(C)CN(Cc1ccc(s1)-c1ccc(N)nc1)S(=O)(=O)Cc1ccccc1          5.82   \n",
            "3  CC(C)CN(Cc2ccc(c1ccc(=O)[nH]c1)s2)S(=O)(=O)Cc3...          5.44   \n",
            "4  CC(C)CN(Cc2ccc(c1ccc(C(N)=O)cc1)cc2)S(=O)(=O)C...          7.48   \n",
            "\n",
            "         reference  \n",
            "0  BMCL 2014, 3891  \n",
            "1   JMC 2015, 5308  \n",
            "2  BMCL 2014, 2182  \n",
            "3  BMCL 2014, 2182  \n",
            "4  BMCL 2014, 3891  \n"
          ]
        }
      ],
      "source": [
        "# Check if file exists before loading\n",
        "if not CSV_FILE:\n",
        "    raise FileNotFoundError(f\"CSV file not found: {CSV_FILE}\")\n",
        "\n",
        "# Load the dataset\n",
        "try:\n",
        "    df = pd.read_csv(CSV_FILE, low_memory=False)\n",
        "    print(f\"Successfully loaded dataset: {df.shape}\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    print(df.head())\n",
        "except Exception as e:\n",
        "    raise IOError(f\"Error loading CSV file {CSV_FILE}: {e}\")\n",
        "\n",
        "# Validate that the dataset has the expected structure\n",
        "if df.empty:\n",
        "    raise ValueError(\"Dataset is empty\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aqs4Yqc0gzw"
      },
      "source": [
        "## 2.3 Define `Boltz-2` functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WsadtHXE0gzw"
      },
      "outputs": [],
      "source": [
        "async def make_nvcf_call(function_url: str,\n",
        "                        data: Dict[str, Any],\n",
        "                        additional_headers: Optional[Dict[str, Any]] = None,\n",
        "                        NVCF_POLL_SECONDS: int = 300,\n",
        "                        MANUAL_TIMEOUT_SECONDS: int = 400) -> Dict:\n",
        "    \"\"\"\n",
        "    Make a call to NVIDIA Cloud Functions using long-polling,\n",
        "    which allows the request to patiently wait if there are many requests in the queue.\n",
        "    \"\"\"\n",
        "    async with httpx.AsyncClient() as client:\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "            \"NVCF-POLL-SECONDS\": f\"{NVCF_POLL_SECONDS}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "            }\n",
        "        if additional_headers is not None:\n",
        "            headers.update(additional_headers)\n",
        "        logger.debug(f\"Headers: {dict(**{h: v for h, v  in headers.items() if 'Authorization' not in h})}\")\n",
        "        # TIMEOUT must be greater than NVCF-POLL-SECONDS\n",
        "        logger.debug(f\"Making NVCF call to {function_url}\")\n",
        "        logger.debug(f\"Data: {data}\")\n",
        "        response = await client.post(function_url,\n",
        "                                     json=data,\n",
        "                                     headers=headers,\n",
        "                                     timeout=MANUAL_TIMEOUT_SECONDS)\n",
        "        logger.debug(f\"NVCF response: {response.status_code, response.headers}\")\n",
        "\n",
        "        if response.status_code == 202:\n",
        "            # Handle 202 Accepted response\n",
        "            task_id = response.headers.get(\"nvcf-reqid\")\n",
        "            while True:\n",
        "                ## Should return in 5 seconds, but we set a manual timeout in 10 just in case\n",
        "                status_response = await client.get(STATUS_URL.format(task_id=task_id),\n",
        "                                                   headers=headers,\n",
        "                                                   timeout=MANUAL_TIMEOUT_SECONDS)\n",
        "                if status_response.status_code == 200:\n",
        "                    return status_response.status_code, status_response\n",
        "                elif status_response.status_code in [400, 401, 404, 422, 500]:\n",
        "                    raise Exception(f\"HTTP {status_response.status_code}: Error while waiting for function: {response.text}\")\n",
        "        elif response.status_code == 200:\n",
        "            return response.status_code, response\n",
        "        else:\n",
        "            raise Exception(f\"HTTP {response.status_code}: {response.text}\")\n",
        "\n",
        "\n",
        "def process_msa_file(msa_file_path: Path = None) -> str:\n",
        "    \"\"\"\n",
        "    Process MSA file and return its content.\n",
        "\n",
        "    Args:\n",
        "        msa_file_path: Path to MSA file (default: global MSA_FILE_PATH)\n",
        "\n",
        "    Returns:\n",
        "        MSA alignment data as string\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If MSA file doesn't exist\n",
        "        IOError: If there's an error reading the file\n",
        "    \"\"\"\n",
        "    if not msa_file_path.exists():\n",
        "        raise FileNotFoundError(f\"MSA file not found: {msa_file_path}\")\n",
        "    try:\n",
        "        with open(msa_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read().strip()\n",
        "            if not content:\n",
        "                raise IOError(f\"MSA file is empty: {msa_file_path}\")\n",
        "            return content\n",
        "    except UnicodeDecodeError as e:\n",
        "        raise IOError(f\"Error decoding MSA file {msa_file_path}: {e}\")\n",
        "    except Exception as e:\n",
        "        raise IOError(f\"Error reading MSA file {msa_file_path}: {e}\")\n",
        "\n",
        "\n",
        "def create_payload(seq: str, smiles: str, msa_content: str) -> Dict[str, Any]:\n",
        "    \"\"\"Create the payload for Boltz2 NIM query.\n",
        "\n",
        "    Args:\n",
        "        seq (str): Protein sequence\n",
        "        smiles: SMILES string for the ligand\n",
        "        msa_content (str, optional): MSA alignment content as string\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: Payload for Boltz2 NIM query\n",
        "    \"\"\"\n",
        "    polymer_data = {\n",
        "        \"id\": \"A\",\n",
        "        \"molecule_type\": \"protein\",\n",
        "        \"sequence\": seq,\n",
        "    }\n",
        "    # Add MSA data if content is provided\n",
        "    if msa_content:\n",
        "        polymer_data[\"msa\"] = {\n",
        "            \"uniref90\": {\n",
        "                \"a3m\": {\n",
        "                    \"alignment\": msa_content,\n",
        "                    \"format\": \"a3m\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    # Construct complete payload using configuration constants\n",
        "    payload = {\n",
        "        \"polymers\": [polymer_data],\n",
        "        \"ligands\": [{\n",
        "            \"smiles\": smiles,\n",
        "            \"id\": \"L1\",\n",
        "            \"predict_affinity\": True\n",
        "        }],\n",
        "        **BOLTZ2_CONFIG\n",
        "    }\n",
        "    return payload\n",
        "\n",
        "\n",
        "async def boltz2_nim_query(sequence: str, smiles: str, msa_file_path: str):\n",
        "    if msa_file_path:\n",
        "        protein_msa = process_msa_file(msa_file_path)\n",
        "    else:\n",
        "        protein_msa = None\n",
        "    # Prepare the request payload\n",
        "    data = create_payload(sequence, smiles, protein_msa)\n",
        "    print(\"Making request...\")\n",
        "    code, response = await make_nvcf_call(function_url=BOLTZ2_ENDPOINT, data=data)\n",
        "    if code == 200:\n",
        "        print(f\"Request succeeded, returned {code}\")\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Request failed, returned {code}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def validate_response(result: Dict[str, Any], seq_id: str) -> tuple[Optional[float], list, list]:\n",
        "    \"\"\"\n",
        "    Validate API response and extract required data.\n",
        "\n",
        "    Args:\n",
        "        result: API response dictionary\n",
        "        seq_id: Protein sequence identifier for error reporting\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (plddt_score, pic50_values, pic50_confidence_values)\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If required data is missing from response\n",
        "    \"\"\"\n",
        "    # Validate affinities data\n",
        "    if 'affinities' not in result:\n",
        "        raise ValueError(f\"Missing 'affinities' key in response for {seq_id}\")\n",
        "    if 'L1' not in result['affinities']:\n",
        "        raise ValueError(f\"Missing 'L1' ligand data in affinities for {seq_id}\")\n",
        "    # Extract pLDDT scores\n",
        "    if 'complex_plddt_scores' not in result or not result['complex_plddt_scores']:\n",
        "        print(f\"Warning: Missing or empty pLDDT scores for {seq_id}\")\n",
        "        plddt_indiv = None\n",
        "    else:\n",
        "        try:\n",
        "            plddt_indiv = float(np.mean(result['complex_plddt_scores']))\n",
        "        except (TypeError, ValueError) as e:\n",
        "            print(f\"Warning: Error calculating pLDDT mean for {seq_id}: {e}\")\n",
        "            plddt_indiv = None\n",
        "    # Extract affinity data\n",
        "    try:\n",
        "        pic50_indiv = result['affinities']['L1']['affinity_pic50']\n",
        "        pic50_conf_indiv = result['affinities']['L1']['affinity_probability_binary']\n",
        "        # Validate that we got lists\n",
        "        if not isinstance(pic50_indiv, list) or not isinstance(pic50_conf_indiv, list):\n",
        "            raise ValueError(f\"Expected lists for affinity data, got {type(pic50_indiv)} and {type(pic50_conf_indiv)}\")\n",
        "    except KeyError as e:\n",
        "        raise ValueError(f\"Missing required affinity data key {e} for {seq_id}\")\n",
        "    return plddt_indiv, pic50_indiv, pic50_conf_indiv\n",
        "\n",
        "\n",
        "async def process_smiles_from_dataframe(df, smiles_field, pic50_field, sequence_string, sequence_id, msa_file_path, sleep_time: int = 30):\n",
        "    \"\"\"\n",
        "    Process SMILES from a DataFrame and run Boltz-2 queries for each one.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing SMILES data\n",
        "        smiles_field: Column name in the DataFrame containing SMILES strings\n",
        "        pic50_field: Column name in the DataFrame containing actual pIC50 values\n",
        "        sequence_string: Protein sequence string for the queries\n",
        "        sequence_id: Protein sequence identifier\n",
        "        msa_file_path: Path to MSA file for the protein\n",
        "        sleep_time: Time to sleep between queries (default: 30 seconds, only required when using the public URL)\n",
        "            NO SLEEP/PAUSE needed when using BOLTZ2 NIM deployed on your own infrastructure\n",
        "    Returns:\n",
        "        Tuple of result lists: (plddt_list, pic50_list, pic50_conf_list, time_list, current_protein_data)\n",
        "    \"\"\"\n",
        "    # Initialize result lists\n",
        "    plddt_list, pic50_list, pic50_conf_list, time_list = [], [], [], []\n",
        "    current_protein_data = []\n",
        "\n",
        "    # Get SMILES and pIC50 lists from DataFrame\n",
        "    smiles_list = df[smiles_field].tolist()\n",
        "    pic50_list_actual = df[pic50_field].tolist()\n",
        "\n",
        "    for smiles, pic50_groundtruth in tqdm(zip(smiles_list, pic50_list_actual)):\n",
        "        # Query the NIM\n",
        "        t0 = perf_counter()\n",
        "        response = await boltz2_nim_query(sequence_string, smiles, msa_file_path)\n",
        "        t1 = perf_counter()\n",
        "        time_indv = round(t1 - t0, 3)\n",
        "\n",
        "        # Extract and validate results\n",
        "        plddt_indiv, pic50_indiv, pic50_conf_indiv = validate_response(response, sequence_id)\n",
        "\n",
        "        # Save results to lists\n",
        "        plddt_list.append(plddt_indiv)\n",
        "        pic50_list.append(pic50_indiv)\n",
        "        pic50_conf_list.append(pic50_conf_indiv)\n",
        "        time_list.append(time_indv)\n",
        "\n",
        "        # Store current ligand data for CSV\n",
        "        current_protein_data.append({\n",
        "            'smiles': smiles,\n",
        "            'uniprot_id': sequence_id,\n",
        "            'fasta_uniprot_seq': sequence_string,\n",
        "            'pic50': pic50_groundtruth,\n",
        "            'boltz2_plddt': round(plddt_indiv, 2) if plddt_indiv is not None else None,\n",
        "            'boltz2_pic50': round(pic50_indiv[0], 2) if pic50_indiv and pic50_indiv[0] is not None else None,\n",
        "            'boltz2_pic50_conf': round(pic50_conf_indiv[0], 2) if pic50_conf_indiv and pic50_conf_indiv[0] is not None else None,\n",
        "            'boltz2_runtime': time_indv\n",
        "        })\n",
        "\n",
        "        # pause for 30 seconds to avoid timeout 422 error (only required when using the public URL)\n",
        "        # NO PAUSE needed when using BOLTZ2 NIM deployed on your own infrastructure\n",
        "        sleep(sleep_time)\n",
        "\n",
        "    return plddt_list, pic50_list, pic50_conf_list, time_list, current_protein_data\n",
        "\n",
        "\n",
        "def process_results_to_dataframe(results):\n",
        "    \"\"\"\n",
        "    Process the results from process_smiles_from_dataframe into a new DataFrame.\n",
        "\n",
        "    Args:\n",
        "        results: Tuple containing (plddt_list, pic50_list, pic50_conf_list, time_list, current_protein_data)\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Processed results as a DataFrame\n",
        "    \"\"\"\n",
        "    _, _, _, _, current_protein_data = results\n",
        "\n",
        "    # Convert current_protein_data to DataFrame\n",
        "    return pd.DataFrame(current_protein_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsaaoBcA0gzx"
      },
      "source": [
        " ## 2.4 Query `Boltz-2` and cast results into a DataFrame\n",
        "\n",
        " ### Using `Boltz-2` NIM in Google Colab environment requires 30 second `sleep` to avoid `422 error`.\n",
        " ### It will run **much faster** on your own infrastructure. No `sleep`/pause needed when running on your own infrastructure. üëç"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAJrsQ-S0gzx",
        "outputId": "186a622e-4389-4a97-ab02-f71d982c1a01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r1it [00:39, 39.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r2it [01:18, 39.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r3it [01:57, 39.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r4it [02:36, 39.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r5it [03:15, 39.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r6it [03:54, 39.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r7it [04:34, 39.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r8it [05:14, 39.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r9it [05:53, 39.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r10it [06:31, 38.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r11it [07:10, 38.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r12it [07:49, 38.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r13it [08:27, 38.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r14it [09:06, 38.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r15it [09:46, 38.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r16it [10:24, 38.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r17it [11:03, 38.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r18it [11:42, 38.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r19it [12:21, 38.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making request...\n",
            "Request succeeded, returned 200\n"
          ]
        }
      ],
      "source": [
        "results = await process_smiles_from_dataframe(df, \"smiles\", \"actual_pIC50\", SEQUENCE_STRING, SEQUENCE_ID, MSA_FILE_PATH, sleep_time=30)\n",
        "results_df = process_results_to_dataframe(results)\n",
        "print(results_df.shape)\n",
        "results_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSZhdwEO0gzx"
      },
      "source": [
        " ## 2.5 Save DataFrame to `CSV` and download to local machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EE2rIt60gzx"
      },
      "outputs": [],
      "source": [
        "df_file_path = f\"{OUTPUT_DIR}/Boltz2_Predictions_for_{uniprot_id}_with_MSA_{MSA_STATUS}_{timestamp}.csv\"\n",
        "\n",
        "results_df.to_csv(df_file_path, index=False)\n",
        "\n",
        "files.download(df_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KqdS9pF0gzx"
      },
      "source": [
        "## 2.6 Plot Results\n",
        "### Actual pIC50 vs Boltz2 Predicted pIC50 with regression lines and R-squared values\n",
        "\n",
        "Uses IBM colorblind-friendly palette: https://www.color-hex.com/color-palette/1044488"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYUYEbaM0gzx"
      },
      "outputs": [],
      "source": [
        "ORANGE = (254/255, 97/255, 0) # from IBM colorblind friendly palette\n",
        "\n",
        "# Create figure\n",
        "plt.figure(figsize=(6, 5))\n",
        "\n",
        "# Plot data and regression lines\n",
        "x_values = results_df['pic50'].values.reshape(-1, 1)\n",
        "y_values = results_df['boltz2_pic50'].values\n",
        "\n",
        "# Remove any NaN values\n",
        "valid_mask = ~(np.isnan(x_values.flatten()) | np.isnan(y_values))\n",
        "x_clean = x_values[valid_mask].reshape(-1, 1)\n",
        "y_clean = y_values[valid_mask]\n",
        "\n",
        "if len(x_clean) > 1:  # Need at least 2 points for regression\n",
        "    # Scatter plot\n",
        "    plt.scatter(x_clean.flatten(),\n",
        "                y_clean,\n",
        "                color=ORANGE,\n",
        "                alpha=0.8,\n",
        "                s=100,\n",
        "                label=f'With MSA (n={len(x_clean)})')\n",
        "\n",
        "    # Fit linear regression\n",
        "    regression_model = LinearRegression()\n",
        "    regression_model.fit(x_clean, y_clean)\n",
        "\n",
        "    # Calculate R-squared\n",
        "    y_predicted = regression_model.predict(x_clean)\n",
        "    r2_score_value = r2_score(y_clean, y_predicted)\n",
        "\n",
        "    # Create regression line\n",
        "    x_range = np.linspace(x_clean.min(), x_clean.max(), 100).reshape(-1, 1)\n",
        "    y_range = regression_model.predict(x_range)\n",
        "\n",
        "    # Plot regression line\n",
        "    plt.plot(x_range.flatten(),\n",
        "             y_range,\n",
        "             color=ORANGE,\n",
        "             linewidth=2,\n",
        "             linestyle='--',\n",
        "             label=f'With MSA R¬≤ = {r2_score_value:.2f}')\n",
        "\n",
        "# Add diagonal reference line (perfect prediction)\n",
        "min_val = min(results_df['pic50'].min(), results_df['boltz2_pic50'].min())\n",
        "max_val = max(results_df['pic50'].max(), results_df['boltz2_pic50'].max())\n",
        "plt.plot([min_val, max_val], [min_val, max_val],\n",
        "         'k--', alpha=0.8, linewidth=1, label='Perfect Prediction')\n",
        "\n",
        "plot_figure_path = f\"{OUTPUT_DIR}/Boltz2_pIC50_vs_Actual_pIC50__for_{uniprot_id}_with_MSA_{MSA_STATUS}_{timestamp}.png\"\n",
        "\n",
        "# Formatting\n",
        "plt.xlabel(r\"Actual pIC$_{50}$\", fontsize=14)\n",
        "plt.ylabel(r\"Boltz-2 Predicted pIC$_{50}$\", fontsize=14)\n",
        "plt.title(r\"Boltz-2 pIC$_{50}$ vs Actual pIC$_{50}$\", fontsize=16)\n",
        "plt.legend(fontsize=12, framealpha=0.9)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tick_params(labelsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(plot_figure_path, dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYkHFMUH0gzx"
      },
      "source": [
        " ## 2.7 Save plot figure to local machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJ6yRifk0gzx"
      },
      "outputs": [],
      "source": [
        "files.download(plot_figure_path)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}