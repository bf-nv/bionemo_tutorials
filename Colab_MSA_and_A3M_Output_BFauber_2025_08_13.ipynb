{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ_5U-JxQQhQ"
      },
      "source": [
        "# Running MSA-Search NIM in Google Colab Environment\n",
        "\n",
        "Runs MSA-Search NIM and saves result as `A3M` format file.\n",
        "\n",
        "MSA-Search NIM: https://build.nvidia.com/colabfold/msa-search\n",
        "\n",
        "13Aug2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxeMs4KwQQhR"
      },
      "source": [
        "## 1.1 Set Up the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B5xoy0RQQhR",
        "outputId": "c5a5c46c-eedf-4b30-aa57-ffaa9131da17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: fastapi[standard] in /usr/local/lib/python3.11/dist-packages (0.116.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx) (0.16.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (0.47.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (4.14.1)\n",
            "Collecting fastapi-cli>=0.0.8 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading fastapi_cli-0.0.8-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: jinja2>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (3.1.6)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (0.0.20)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard])\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (0.35.0)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard])\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (0.16.0)\n",
            "Collecting rich-toolkit>=0.14.8 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading rich_toolkit-0.15.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading fastapi_cloud_cli-0.1.5-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.1.5->fastapi[standard]) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi[standard]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi[standard]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi[standard]) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx) (1.3.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.12.0->uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (8.2.1)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (6.0.2)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (15.0.1)\n",
            "Collecting rignore>=0.5.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading rignore-0.6.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (2.34.1)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (13.9.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (2.19.2)\n",
            "Requirement already satisfied: urllib3>=1.26.11 in /usr/local/lib/python3.11/dist-packages (from sentry-sdk>=2.20.0->fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (0.1.2)\n",
            "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading fastapi_cli-0.0.8-py3-none-any.whl (10 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi_cloud_cli-0.1.5-py3-none-any.whl (18 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading rich_toolkit-0.15.0-py3-none-any.whl (29 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rignore-0.6.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (950 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.6/950.6 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvloop, rignore, python-dotenv, httptools, dnspython, watchfiles, email-validator, rich-toolkit, fastapi-cloud-cli, fastapi-cli\n",
            "Successfully installed dnspython-2.7.0 email-validator-2.2.0 fastapi-cli-0.0.8 fastapi-cloud-cli-0.1.5 httptools-0.6.4 python-dotenv-1.1.1 rich-toolkit-0.15.0 rignore-0.6.4 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy seaborn matplotlib httpx \"fastapi[standard]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-Kwwr0x8QQhR"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import requests\n",
        "import re\n",
        "import shutil\n",
        "from google.colab import userdata\n",
        "\n",
        "import asyncio\n",
        "from typing import Any, Dict, Optional\n",
        "from pathlib import Path\n",
        "from enum import StrEnum\n",
        "import logging\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvI9Bw7rQQhR"
      },
      "source": [
        "## 1.2 Set Up `output` Directory and `API_KEY`\n",
        "\n",
        "**NOTE:** Be sure to follow the steps in the README to embed your NVIDIA `API_KEY` into your Google Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ylQVJvcvQQhR"
      },
      "outputs": [],
      "source": [
        "def prepare_output_directory(output):\n",
        "    \"\"\"\n",
        "    Prepare the output directory\n",
        "    output: str, the output directory\n",
        "    return: None\n",
        "    \"\"\"\n",
        "    # Overwrite the output directory\n",
        "    if os.path.exists(output):\n",
        "        shutil.rmtree(output)\n",
        "    os.makedirs(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9H2iqQG4QQhR"
      },
      "outputs": [],
      "source": [
        "API_KEY = userdata.get('API_KEY')\n",
        "\n",
        "# Prepare `output_dir` for saving files\n",
        "output_dir = \"/content/output\"\n",
        "prepare_output_directory(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoj321rTQQhR"
      },
      "source": [
        "#### Define Protein Sequence and Databases to use for MSA-Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hKRtkuWYQQhS"
      },
      "outputs": [],
      "source": [
        "sequence = \"MHHHHHHGENLYFQGSAPYASLTEIEHLVQSVCKSYRETCQLRLEDLLRQRSNIFSREEVTGYQRKSMWEMWERCAHHLTEAIQYVVEFAKRLSGFMELCQNDQIVLLKAGAMEVVLVRMCRAYNADNRTVFFEGKYGGMELFRALGCSELISSIFDFSHSLSALHFSEDEIALYTALVLINAHRPGLQEKRKVEQLQYNLELAFHHHLCKTHRQSILAKLPPKGKLRSLCSQHVERLQIFQHLHPIVVQAAFPPLYKELFSGNS\"\n",
        "\n",
        "databases = ['Uniref30_2302', 'colabfold_envdb_202108', 'PDB70_220313']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7iH2uQvQQhS"
      },
      "source": [
        "## 1.3 Set Up and Run `MSA-Search`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqazKTWbQQhS"
      },
      "outputs": [],
      "source": [
        "msa_search_url = \"https://health.api.nvidia.com/v1/biology/colabfold/msa-search/predict\"\n",
        "payload = {\n",
        "    \"sequence\": sequence,\n",
        "    \"databases\": databases,\n",
        "    \"e_value\": 0.0001,\n",
        "    \"iterations\": 1,\n",
        "    \"max_msa_sequences\": 10000,\n",
        "    \"run_structural_template_search\": False,\n",
        "    \"output_alignment_formats\": [\"a3m\"],\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "    \"content-type\": \"application/json\",\n",
        "    \"NVCF-POLL-SECONDS\": \"300\",\n",
        "}\n",
        "# Call MSA-Search NIM\n",
        "response = requests.post(msa_search_url, json=payload, headers=headers)\n",
        "msa_response_dict = response.json()\n",
        "print(f\"MSA response : \\n {msa_response_dict}\")\n",
        "\n",
        "with open('raw_msa_output.json', 'w') as json_file:\n",
        "    json.dump(msa_response_dict, json_file, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGfAibuMQQhS"
      },
      "source": [
        "## 1.4 Merge and Sort Alignments from `MSA-Search` into a Single `A3M` File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vbwqlUa-QQhS"
      },
      "outputs": [],
      "source": [
        "def parse_sequences(input_string, n, sequence):\n",
        "    \"\"\"\n",
        "    Parse the output of alignments from the MSA-Search NIM to be used downstream\n",
        "\n",
        "    Args:\n",
        "        input_string (str): The output file of alignments in a string format\n",
        "        n (int): The amount of alignments to return from the output when parsing\n",
        "        sequence (str): The query sequence for alignment\n",
        "\n",
        "    Returns:\n",
        "        list: A list of alignment identifiers and sequences, starting with the query,\n",
        "              where the amount of sequences is given by n\n",
        "    \"\"\"\n",
        "    # Output is parsed to have a line for the sequence id and sequence itself so `n` returns correlates to n*2 lines\n",
        "    n = n * 2\n",
        "\n",
        "    # First, handle the `Query` block separately\n",
        "    lines = input_string.strip().split('\\n')\n",
        "\n",
        "    # Now process the rest of the lines\n",
        "    remaining_string = \"\\n\".join(lines[:])\n",
        "\n",
        "    # Regex to find blocks starting with `>` and then followed by a sequence.\n",
        "    pattern = re.compile(r'\\n>(.*?)\\n(.*?)(?=\\n>|\\Z)', re.DOTALL)\n",
        "\n",
        "    matches = pattern.finditer(remaining_string)\n",
        "\n",
        "    output_list = []\n",
        "    output_list_to_order = []\n",
        "\n",
        "    for num_match, match in enumerate(matches):\n",
        "        # The name is the first capturing group, split by tab and take the first part\n",
        "        name_full = match.group(1).split('\\t')[0]\n",
        "        SW_score = match.group(1).split('\\t')[1]\n",
        "\n",
        "        # The sequence is the second capturing group\n",
        "        sequence_raw = match.group(2).strip()\n",
        "        sequence = ''.join(char for char in sequence_raw if char.isupper() or not char.isalpha())\n",
        "\n",
        "        # Store the aligned sequence in the list of outputs by name, sequence, Smith-Waterman score\n",
        "        output_list_to_order.append((f'>{name_full}', sequence, int(SW_score)))\n",
        "\n",
        "    output_lines = output_list_to_order[:n]\n",
        "\n",
        "    return output_lines\n",
        "\n",
        "\n",
        "def write_alignments_to_a3m(alignments_data, output_file_path, description=\"MSA alignments\"):\n",
        "    \"\"\"\n",
        "    Write alignment data to a3M format file.\n",
        "\n",
        "    Args:\n",
        "        alignments_data: Either a list of alternating headers/sequences or a string containing alignments\n",
        "        output_file_path (str): Path for the output a3M file\n",
        "        description (str): Description for the file\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the created a3M file\n",
        "    \"\"\"\n",
        "    output_path = Path(output_file_path)\n",
        "\n",
        "    # Handle both list and string input formats\n",
        "    if isinstance(alignments_data, list):\n",
        "        alignments_string = '\\n'.join(alignments_data)\n",
        "    elif isinstance(alignments_data, str):\n",
        "        alignments_string = alignments_data\n",
        "    else:\n",
        "        raise ValueError(\"alignments_data must be either a list or string\")\n",
        "\n",
        "    # Count sequences for reporting\n",
        "    sequence_count = alignments_string.count('>')\n",
        "\n",
        "    print(f\"Writing {sequence_count} sequences to a3M format: {output_path}\")\n",
        "\n",
        "    try:\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            # Write the alignments\n",
        "            f.write(alignments_string)\n",
        "\n",
        "            # Ensure file ends with newline\n",
        "            if not alignments_string.endswith('\\n'):\n",
        "                f.write('\\n')\n",
        "\n",
        "        # Verify the file was created successfully\n",
        "        if output_path.exists():\n",
        "            file_size = output_path.stat().st_size\n",
        "            print(f\"Successfully created a3M file:\")\n",
        "            print(f\"File: {output_path}\")\n",
        "            print(f\"Size: {file_size:,} bytes\")\n",
        "            print(f\"Sequences: {sequence_count}\")\n",
        "\n",
        "            return str(output_path)\n",
        "        else:\n",
        "            raise IOError(f\"Failed to create file {output_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing a3M file: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def process_msa_alignments(msa_response_dict, databases, sequence, max_sequences_per_db=10000, output_file=\"merged_alignments_protein.a3m\"):\n",
        "    \"\"\"\n",
        "    Process MSA alignments from multiple databases and merge them into A3M format.\n",
        "\n",
        "    Args:\n",
        "        msa_response_dict (dict): MSA response data containing alignments\n",
        "        databases (list): List of database names to process\n",
        "        sequence (str): Query sequence for alignment\n",
        "        max_sequences_per_db (int): Maximum number of sequences to parse per database\n",
        "        output_file (str): Output A3M file path\n",
        "\n",
        "    Returns:\n",
        "        tuple: (merged_alignments_protein, a3m_file_path)\n",
        "            - merged_alignments_protein: List of merged alignments\n",
        "            - a3m_file_path: Path to the created A3M file\n",
        "    \"\"\"\n",
        "    all_parsed_dataset_output = []\n",
        "\n",
        "    for num_done, database in enumerate(databases):\n",
        "        print(f\"Parsing results from database: {database}\")\n",
        "\n",
        "        # Pull string of alignments stored in json output for specific dataset\n",
        "        a3m_dict_msa_search = msa_response_dict['alignments'][database]['a3m']['alignment']\n",
        "\n",
        "        a3m_dict_msa_search_parsed = parse_sequences(a3m_dict_msa_search, max_sequences_per_db, sequence)\n",
        "\n",
        "        num_sequences_aligned = (len(a3m_dict_msa_search_parsed))\n",
        "        print(f\"Number of sequences aligned: {num_sequences_aligned}\")\n",
        "\n",
        "        all_parsed_dataset_output.extend(a3m_dict_msa_search_parsed)\n",
        "\n",
        "    # Sort all the alignments based off of the alignment score\n",
        "    all_parsed_dataset_output.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    # Now that the alignments across all datasets are sorted, reformat each entry to name and sequence\n",
        "    sorted_parsed_output_formatted = []\n",
        "    for align_tuple in all_parsed_dataset_output:\n",
        "        sorted_parsed_output_formatted.append(align_tuple[0])\n",
        "        sorted_parsed_output_formatted.append(align_tuple[1])\n",
        "\n",
        "    merged_alignments_protein = [f\">query_sequence\\n{sequence}\"]\n",
        "    merged_alignments_protein.extend(sorted_parsed_output_formatted)\n",
        "\n",
        "    print(f\"Total merged alignments: {len(merged_alignments_protein)}\")\n",
        "\n",
        "    # Write merged_alignments_protein to a3M format\n",
        "    a3m_file_path = write_alignments_to_a3m(\n",
        "        merged_alignments_protein,\n",
        "        output_file,\n",
        "        description=f\"Merged protein alignments from MSA-Search NIM ({', '.join(databases)})\"\n",
        "    )\n",
        "\n",
        "    return merged_alignments_protein, a3m_file_path\n",
        "\n",
        "\n",
        "def write_filtered_a3m(alignments_data, output_file_path, max_sequences=None, min_length=None, description=\"Filtered MSA alignments\"):\n",
        "    \"\"\"\n",
        "    Write alignment data to a3M format with optional filtering.\n",
        "\n",
        "    Args:\n",
        "        alignments_data: String containing alignments in FASTA-like format\n",
        "        output_file_path (str): Path for the output a3M file\n",
        "        max_sequences (int, optional): Maximum number of sequences to include\n",
        "        min_length (int, optional): Minimum sequence length (excluding gaps)\n",
        "        description (str): Description for the file\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the created a3M file\n",
        "    \"\"\"\n",
        "    output_path = Path(output_file_path)\n",
        "\n",
        "    # Parse sequences from the input data\n",
        "    if isinstance(alignments_data, str):\n",
        "        lines = alignments_data.strip().split('\\n')\n",
        "    else:\n",
        "        lines = '\\n'.join(alignments_data).strip().split('\\n')\n",
        "\n",
        "    sequences = []\n",
        "    current_header = None\n",
        "    current_sequence = \"\"\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith('>'):\n",
        "            # Save previous sequence if it exists\n",
        "            if current_header is not None:\n",
        "                sequences.append((current_header, current_sequence))\n",
        "            current_header = line\n",
        "            current_sequence = \"\"\n",
        "        else:\n",
        "            current_sequence += line\n",
        "\n",
        "    # Don't forget the last sequence\n",
        "    if current_header is not None:\n",
        "        sequences.append((current_header, current_sequence))\n",
        "\n",
        "    print(f\"Parsed {len(sequences)} sequences from input data\")\n",
        "\n",
        "    # Apply filters\n",
        "    filtered_sequences = []\n",
        "\n",
        "    for header, sequence in sequences:\n",
        "        # Apply minimum length filter (count non-gap characters)\n",
        "        if min_length is not None:\n",
        "            non_gap_length = len(sequence.replace('-', '').replace('.', ''))\n",
        "            if non_gap_length < min_length:\n",
        "                continue\n",
        "\n",
        "        filtered_sequences.append((header, sequence))\n",
        "\n",
        "        # Apply maximum sequences limit\n",
        "        if max_sequences is not None and len(filtered_sequences) >= max_sequences:\n",
        "            break\n",
        "\n",
        "    print(f\"After filtering: {len(filtered_sequences)} sequences\")\n",
        "    if max_sequences:\n",
        "        print(f\"Max sequences limit: {max_sequences}\")\n",
        "    if min_length:\n",
        "        print(f\"Min length filter: {min_length}\")\n",
        "\n",
        "    # Write to a3M format\n",
        "    try:\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            # Write sequences\n",
        "            for header, sequence in filtered_sequences:\n",
        "                f.write(f\"{header}\\n{sequence}\\n\")\n",
        "\n",
        "        # Report success\n",
        "        file_size = output_path.stat().st_size\n",
        "        print(f\"Successfully created filtered a3M file:\")\n",
        "        print(f\"File: {output_path}\")\n",
        "        print(f\"Size: {file_size:,} bytes\")\n",
        "        print(f\"Sequences: {len(filtered_sequences)}\")\n",
        "\n",
        "        return str(output_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing filtered a3M file: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def analyze_a3m_file(file_path):\n",
        "    \"\"\"\n",
        "    Analyze an a3M file and provide statistics.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the a3M file\n",
        "    \"\"\"\n",
        "    file_path = Path(file_path)\n",
        "\n",
        "    if not file_path.exists():\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Analyzing a3M file: {file_path.name}\")\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        # Count statistics\n",
        "        total_lines = len(lines)\n",
        "        comment_lines = sum(1 for line in lines if line.startswith('#'))\n",
        "        sequence_headers = sum(1 for line in lines if line.startswith('>'))\n",
        "        sequence_lines = total_lines - comment_lines - sequence_headers\n",
        "\n",
        "        # Calculate sequence lengths\n",
        "        sequence_lengths = []\n",
        "        current_sequence = \"\"\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line.startswith('#'):\n",
        "                continue\n",
        "            elif line.startswith('>'):\n",
        "                if current_sequence:\n",
        "                    sequence_lengths.append(len(current_sequence))\n",
        "                current_sequence = \"\"\n",
        "            else:\n",
        "                current_sequence += line\n",
        "\n",
        "        # Don't forget the last sequence\n",
        "        if current_sequence:\n",
        "            sequence_lengths.append(len(current_sequence))\n",
        "\n",
        "        # File statistics\n",
        "        file_size = file_path.stat().st_size\n",
        "\n",
        "        print(f\"File Statistics:\")\n",
        "        print(f\"File size: {file_size:,} bytes\")\n",
        "        print(f\"Total lines: {total_lines}\")\n",
        "        print(f\"Comment lines: {comment_lines}\")\n",
        "        print(f\"Sequence headers: {sequence_headers}\")\n",
        "        print(f\"Sequence lines: {sequence_lines}\")\n",
        "\n",
        "        if sequence_lengths:\n",
        "            avg_length = sum(sequence_lengths) / len(sequence_lengths)\n",
        "            min_length = min(sequence_lengths)\n",
        "            max_length = max(sequence_lengths)\n",
        "\n",
        "            print(f\"Sequence Statistics:\")\n",
        "            print(f\"Number of sequences: {len(sequence_lengths)}\")\n",
        "            print(f\"Average length: {avg_length:.1f}\")\n",
        "            print(f\"Length range: {min_length} - {max_length}\")\n",
        "\n",
        "            # Show first sequence as example\n",
        "            with open(file_path, 'r') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Find first sequence\n",
        "            lines = content.split('\\n')\n",
        "            for i, line in enumerate(lines):\n",
        "                if line.startswith('>') and not line.startswith('#'):\n",
        "                    header = line\n",
        "                    sequence = \"\"\n",
        "                    j = i + 1\n",
        "                    while j < len(lines) and not lines[j].startswith('>'):\n",
        "                        if not lines[j].startswith('#'):\n",
        "                            sequence += lines[j].strip()\n",
        "                        j += 1\n",
        "\n",
        "                    print(f\"First sequence example:\")\n",
        "                    print(f\"Header: {header}\")\n",
        "                    print(f\"Length: {len(sequence)}\")\n",
        "                    print(f\"Preview: {sequence[:80]}{'...' if len(sequence) > 80 else ''}\")\n",
        "                    break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing file: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv6SgzR_KUSm"
      },
      "source": [
        "### Parse the MSA alignment results to merge results from all datasets into a single `A3M` format file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DnG5iJQwKUSm",
        "outputId": "634d02cc-78cf-4a90-c8d4-66deffdbbded",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing results from database: Uniref30_2302\n",
            "Number of sequences aligned: 100\n",
            "Parsing results from database: colabfold_envdb_202108\n",
            "Number of sequences aligned: 100\n",
            "Parsing results from database: PDB70_220313\n",
            "Number of sequences aligned: 88\n",
            "Total merged alignments: 577\n",
            "Writing 289 sequences to a3M format: merged_alignments_protein.a3m\n",
            "Successfully created a3M file:\n",
            "File: merged_alignments_protein.a3m\n",
            "Size: 81,161 bytes\n",
            "Sequences: 289\n"
          ]
        }
      ],
      "source": [
        "merged_alignments_protein, a3m_file_path = process_msa_alignments(\n",
        "    msa_response_dict,\n",
        "    databases,\n",
        "    sequence,\n",
        "    max_sequences_per_db=10000,\n",
        "    output_file=\"merged_alignments_protein.a3m\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C0eZrbyKUSm"
      },
      "source": [
        "### Download the `A3M` file to your local machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "s2Rk2ySRKUSm",
        "outputId": "228f01d6-62ff-40d1-e1a0-044f59bd5658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a5e6f69b-bc77-4742-827b-bab555e11d10\", \"merged_alignments_protein.a3m\", 81161)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"raw_msa_output.json\")\n",
        "files.download(\"merged_alignments_protein.a3m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNrP_9qZKUSm"
      },
      "source": [
        "## 1.5 Analyze the `A3M` Format File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sg5UnqgsKUSm"
      },
      "outputs": [],
      "source": [
        "# Analyze all created a3M files\n",
        "print(\"=\" * 60)\n",
        "print(\"A3M FILE ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "files_to_analyze = [\n",
        "    \"merged_alignments_protein.a3m\",\n",
        "]\n",
        "\n",
        "for file_name in files_to_analyze:\n",
        "    if Path(file_name).exists():\n",
        "        analyze_a3m_file(file_name)\n",
        "        print(\"-\" * 40)\n",
        "    else:\n",
        "        print(f\"File not found: {file_name}\")\n",
        "        print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbFTA0MbKUSm"
      },
      "source": [
        "## 1.6 Visualize the MSA Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ifqlk2vKUSm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import Rectangle\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "\n",
        "def visualize_msa_alignment(merged_alignments_protein, top_n=5, figsize=(20, 12)):\n",
        "    \"\"\"\n",
        "    Visualize multiple sequence alignment results emphasizing top-5 residues at each position.\n",
        "\n",
        "    Args:\n",
        "        merged_alignments_protein (list): List of alternating headers and sequences\n",
        "        top_n (int): Number of top residues to highlight at each position\n",
        "        figsize (tuple): Figure size for the plot\n",
        "    \"\"\"\n",
        "\n",
        "    # Parse the alignments into a more structured format\n",
        "    sequences = []\n",
        "    headers = []\n",
        "\n",
        "    for i, item in enumerate(merged_alignments_protein):\n",
        "        if i % 2 == 0:  # Header line\n",
        "            headers.append(item)\n",
        "        else:  # Sequence line\n",
        "            sequences.append(item)\n",
        "\n",
        "    # Get the query sequence (first sequence)\n",
        "    query_sequence = sequences[0]\n",
        "    alignment_length = len(query_sequence)\n",
        "\n",
        "    print(f\"Alignment length: {alignment_length}\")\n",
        "    print(f\"Number of sequences: {len(sequences)}\")\n",
        "    print(f\"Query sequence: {query_sequence}\")\n",
        "\n",
        "    # Create position-wise residue frequency analysis\n",
        "    position_residues = {}\n",
        "    position_gaps = {}\n",
        "\n",
        "    for pos in range(alignment_length):\n",
        "        residues_at_pos = []\n",
        "        gaps_at_pos = 0\n",
        "\n",
        "        for seq in sequences:\n",
        "            if pos < len(seq):\n",
        "                residue = seq[pos]\n",
        "                if residue == '-' or residue == '.':\n",
        "                    gaps_at_pos += 1\n",
        "                else:\n",
        "                    residues_at_pos.append(residue)\n",
        "\n",
        "        # Count residue frequencies\n",
        "        residue_counts = Counter(residues_at_pos)\n",
        "        position_residues[pos] = residue_counts\n",
        "        position_gaps[pos] = gaps_at_pos\n",
        "\n",
        "    # Create the visualization\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=figsize,\n",
        "                                         gridspec_kw={'height_ratios': [1, 2, 1]})\n",
        "\n",
        "    # 1. Top panel: Query sequence with position numbers\n",
        "    ax1.set_title('Query Sequence with Position Numbers', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlim(0, alignment_length)\n",
        "    ax1.set_ylim(0, 1)\n",
        "\n",
        "    # Add position numbers every 10 positions\n",
        "    for pos in range(0, alignment_length, 10):\n",
        "        ax1.text(pos, 0.5, str(pos+1), ha='center', va='center', fontsize=8,\n",
        "                bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=\"lightblue\", alpha=0.7))\n",
        "\n",
        "    # Add query sequence residues\n",
        "    for pos, residue in enumerate(query_sequence):\n",
        "        color = 'red' if residue == '-' else 'blue'\n",
        "        ax1.text(pos, 0.2, residue, ha='center', va='center', fontsize=10,\n",
        "                fontweight='bold', color=color)\n",
        "\n",
        "    ax1.set_xticks([])\n",
        "    ax1.set_yticks([])\n",
        "    ax1.spines['top'].set_visible(False)\n",
        "    ax1.spines['right'].set_visible(False)\n",
        "    ax1.spines['left'].set_visible(False)\n",
        "    ax1.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # 2. Middle panel: Top-N residue frequency heatmap\n",
        "    ax2.set_title(f'Top-{top_n} Residue Frequencies at Each Position', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Prepare data for heatmap\n",
        "    top_residues = set()\n",
        "    for pos in range(alignment_length):\n",
        "        if pos in position_residues:\n",
        "            top_residues.update([res for res, _ in position_residues[pos].most_common(top_n)])\n",
        "\n",
        "    top_residues = sorted(list(top_residues))\n",
        "    heatmap_data = np.zeros((len(top_residues), alignment_length))\n",
        "\n",
        "    for pos in range(alignment_length):\n",
        "        if pos in position_residues:\n",
        "            residue_counts = position_residues[pos]\n",
        "            total_non_gaps = sum(residue_counts.values())\n",
        "            if total_non_gaps > 0:\n",
        "                for i, residue in enumerate(top_residues):\n",
        "                    if residue in residue_counts:\n",
        "                        heatmap_data[i, pos] = residue_counts[residue] / total_non_gaps\n",
        "\n",
        "    # Create heatmap\n",
        "    im = ax2.imshow(heatmap_data, cmap='YlOrRd', aspect='auto', interpolation='nearest')\n",
        "\n",
        "    # Set labels\n",
        "    ax2.set_yticks(range(len(top_residues)))\n",
        "    ax2.set_yticklabels(top_residues)\n",
        "    ax2.set_ylabel('Residues', fontsize=12)\n",
        "\n",
        "    # Set x-axis labels every 10 positions\n",
        "    ax2.set_xticks(range(0, alignment_length, 10))\n",
        "    ax2.set_xticklabels([str(i+1) for i in range(0, alignment_length, 10)])\n",
        "    ax2.set_xlabel('Position', fontsize=12)\n",
        "\n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(im, ax=ax2, shrink=0.8)\n",
        "    cbar.set_label('Frequency', fontsize=10)\n",
        "\n",
        "    # 3. Bottom panel: Gap analysis and conservation score\n",
        "    ax3.set_title('Gap Analysis and Conservation Score', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Calculate conservation score (percentage of most common residue at each position)\n",
        "    conservation_scores = []\n",
        "    for pos in range(alignment_length):\n",
        "        if pos in position_residues and position_residues[pos]:\n",
        "            most_common_count = max(position_residues[pos].values())\n",
        "            total_non_gaps = sum(position_residues[pos].values())\n",
        "            if total_non_gaps > 0:\n",
        "                conservation_scores.append(most_common_count / total_non_gaps)\n",
        "            else:\n",
        "                conservation_scores.append(0)\n",
        "        else:\n",
        "            conservation_scores.append(0)\n",
        "\n",
        "    # Plot conservation scores\n",
        "    x_positions = range(alignment_length)\n",
        "    ax3.plot(x_positions, conservation_scores, 'b-', linewidth=2, alpha=0.8, label='Conservation Score')\n",
        "\n",
        "    # Plot gap percentages\n",
        "    gap_percentages = [position_gaps[pos] / len(sequences) * 100 for pos in range(alignment_length)]\n",
        "    ax3.plot(x_positions, gap_percentages, 'r--', linewidth=2, alpha=0.8, label='Gap Percentage')\n",
        "\n",
        "    # Add horizontal line at 50% conservation\n",
        "    ax3.axhline(y=0.5, color='gray', linestyle=':', alpha=0.7, label='50% Conservation Threshold')\n",
        "\n",
        "    ax3.set_xlabel('Position', fontsize=12)\n",
        "    ax3.set_ylabel('Score/Percentage', fontsize=12)\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # Set x-axis labels every 10 positions\n",
        "    ax3.set_xticks(range(0, alignment_length, 10))\n",
        "    ax3.set_xticklabels([str(i+1) for i in range(0, alignment_length, 10)])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed statistics\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DETAILED ALIGNMENT STATISTICS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Position-wise top residues\n",
        "    print(f\"\\nTop-{top_n} residues at key positions:\")\n",
        "    key_positions = [0, alignment_length//4, alignment_length//2, 3*alignment_length//4, alignment_length-1]\n",
        "\n",
        "    for pos in key_positions:\n",
        "        if pos < alignment_length:\n",
        "            print(f\"\\nPosition {pos+1}:\")\n",
        "            if pos in position_residues:\n",
        "                top_res = position_residues[pos].most_common(top_n)\n",
        "                for residue, count in top_res:\n",
        "                    percentage = (count / sum(position_residues[pos].values())) * 100\n",
        "                    print(f\"  {residue}: {count} ({percentage:.1f}%)\")\n",
        "            else:\n",
        "                print(\"  No data available\")\n",
        "\n",
        "    # Overall statistics\n",
        "    total_gaps = sum(position_gaps.values())\n",
        "    total_positions = alignment_length * len(sequences)\n",
        "    gap_percentage = (total_gaps / total_positions) * 100\n",
        "\n",
        "    print(f\"\\nOverall Statistics:\")\n",
        "    print(f\"Total positions: {total_positions}\")\n",
        "    print(f\"Total gaps: {total_gaps}\")\n",
        "    print(f\"Overall gap percentage: {gap_percentage:.2f}%\")\n",
        "\n",
        "    # Most conserved positions\n",
        "    conservation_by_position = [(pos, conservation_scores[pos]) for pos in range(alignment_length)]\n",
        "    conservation_by_position.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print(f\"\\nTop 10 most conserved positions:\")\n",
        "    for i, (pos, score) in enumerate(conservation_by_position[:10]):\n",
        "        if score > 0:\n",
        "            print(f\"  Position {pos+1}: {score:.3f} ({score*100:.1f}%)\")\n",
        "\n",
        "    return fig, (ax1, ax2, ax3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66CvP2A1KUSm",
        "outputId": "a1a54088-5b7c-4c7f-c53d-57b240f375dd"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'merged_alignments_protein' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m visualize_msa_alignment(\u001b[43mmerged_alignments_protein\u001b[49m, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'merged_alignments_protein' is not defined"
          ]
        }
      ],
      "source": [
        "fig, axes = visualize_msa_alignment(merged_alignments_protein, top_n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKLztq8zQQhS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}