{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ_5U-JxQQhQ"
      },
      "source": [
        "# Running MSA-Search NIM in Google Colab Environment\n",
        "\n",
        "Runs MSA-Search NIM and saves result as `A3M` format file.\n",
        "\n",
        "MSA-Search NIM: https://build.nvidia.com/colabfold/msa-search\n",
        "\n",
        "13Aug2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxeMs4KwQQhR"
      },
      "source": [
        "## 1.1 Set Up the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B5xoy0RQQhR",
        "outputId": "2a4750bf-0d48-4734-bb93-df31929cd349"
      },
      "outputs": [],
      "source": [
        "!pip install pandas numpy seaborn matplotlib httpx \"fastapi[standard]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Kwwr0x8QQhR"
      },
      "outputs": [],
      "source": [
        "import json, os, requests, re, shutil\n",
        "from google.colab import userdata\n",
        "\n",
        "from typing import Any, Dict\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvI9Bw7rQQhR"
      },
      "source": [
        "## 1.2 Set Up `output` Directory and `API_KEY`\n",
        "\n",
        "**NOTE:** Be sure to follow the steps in the README to embed your NVIDIA `API_KEY` into your Google Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylQVJvcvQQhR"
      },
      "outputs": [],
      "source": [
        "def prepare_output_directory(output):\n",
        "    \"\"\"\n",
        "    Prepare the output directory\n",
        "    output: str, the output directory\n",
        "    return: None\n",
        "    \"\"\"\n",
        "    # Overwrite the output directory\n",
        "    if os.path.exists(output):\n",
        "        shutil.rmtree(output)\n",
        "    os.makedirs(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H2iqQG4QQhR"
      },
      "outputs": [],
      "source": [
        "API_KEY = userdata.get('API_KEY')\n",
        "\n",
        "# Prepare `output_dir` for saving files\n",
        "output_dir = \"/content/output\"\n",
        "prepare_output_directory(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoj321rTQQhR"
      },
      "source": [
        "#### Define Protein Sequence and Databases to use for MSA-Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKRtkuWYQQhS"
      },
      "outputs": [],
      "source": [
        "sequence = \"MHHHHHHGENLYFQGSAPYASLTEIEHLVQSVCKSYRETCQLRLEDLLRQRSNIFSREEVTGYQRKSMWEMWERCAHHLTEAIQYVVEFAKRLSGFMELCQNDQIVLLKAGAMEVVLVRMCRAYNADNRTVFFEGKYGGMELFRALGCSELISSIFDFSHSLSALHFSEDEIALYTALVLINAHRPGLQEKRKVEQLQYNLELAFHHHLCKTHRQSILAKLPPKGKLRSLCSQHVERLQIFQHLHPIVVQAAFPPLYKELFSGNS\"\n",
        "\n",
        "databases = ['Uniref30_2302', 'colabfold_envdb_202108', 'PDB70_220313']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7iH2uQvQQhS"
      },
      "source": [
        "## 1.3 Set Up and Run `MSA-Search`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqazKTWbQQhS",
        "outputId": "7677c4cb-b7ce-4446-86bf-6d3d25eef456"
      },
      "outputs": [],
      "source": [
        "msa_search_url = \"https://health.api.nvidia.com/v1/biology/colabfold/msa-search/predict\"\n",
        "payload = {\n",
        "    \"sequence\": sequence,\n",
        "    \"databases\": databases,\n",
        "    \"e_value\": 0.0001,\n",
        "    \"iterations\": 1,\n",
        "    \"max_msa_sequences\": 10000,\n",
        "    \"run_structural_template_search\": False,\n",
        "    \"output_alignment_formats\": [\"a3m\"],\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "    \"content-type\": \"application/json\",\n",
        "    \"NVCF-POLL-SECONDS\": \"300\",\n",
        "}\n",
        "# Call MSA-Search NIM\n",
        "response = requests.post(msa_search_url, json=payload, headers=headers)\n",
        "msa_response_dict = response.json()\n",
        "print(f\"MSA response : \\n {msa_response_dict}\")\n",
        "\n",
        "with open('msa_output.json', 'w') as json_file:\n",
        "    json.dump(msa_response_dict, json_file, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGfAibuMQQhS"
      },
      "source": [
        "## 1.4 Merge and Sort Alignments from `MSA-Search` into a Single `A3M` File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbwqlUa-QQhS"
      },
      "outputs": [],
      "source": [
        "def parse_sequences(input_string, n, sequence):\n",
        "    \"\"\"\n",
        "    Parse the output of alignments from the MSA-Search NIM to be used downstream\n",
        "    \n",
        "    Args:\n",
        "        input_string (str): The output file of alignments in a string format\n",
        "        n (int): The amount of alignments to return from the output when parsing\n",
        "        sequence (str): The query sequence for alignment\n",
        "    \n",
        "    Returns:\n",
        "        list: A list of alignment identifiers and sequences, starting with the query,\n",
        "              where the amount of sequences is given by n\n",
        "    \"\"\"\n",
        "    # Output is parsed to have a line for the sequence id and sequence itself so `n` returns correlates to n*2 lines\n",
        "    n = n * 2\n",
        "\n",
        "    # First, handle the `Query` block separately\n",
        "    lines = input_string.strip().split('\\n')\n",
        "\n",
        "    # Now process the rest of the lines\n",
        "    remaining_string = \"\\n\".join(lines[:])\n",
        "\n",
        "    # Regex to find blocks starting with `>` and then followed by a sequence.\n",
        "    pattern = re.compile(r'\\n>(.*?)\\n(.*?)(?=\\n>|\\Z)', re.DOTALL)\n",
        "\n",
        "    matches = pattern.finditer(remaining_string)\n",
        "\n",
        "    output_list = []\n",
        "    output_list_to_order = []\n",
        "\n",
        "    for num_match, match in enumerate(matches):\n",
        "        # The name is the first capturing group, split by tab and take the first part\n",
        "        name_full = match.group(1).split('\\t')[0]\n",
        "        SW_score = match.group(1).split('\\t')[1]\n",
        "\n",
        "        # The sequence is the second capturing group\n",
        "        sequence_raw = match.group(2).strip()\n",
        "        sequence = ''.join(char for char in sequence_raw if char.isupper() or not char.isalpha())\n",
        "\n",
        "        # Store the aligned sequence in the list of outputs by name, sequence, Smith-Waterman score\n",
        "        output_list_to_order.append((f'>{name_full}', sequence, int(SW_score)))\n",
        "\n",
        "    output_lines = output_list_to_order[:n]\n",
        "\n",
        "    return output_lines\n",
        "\n",
        "\n",
        "def write_alignments_to_a3m(alignments_data, output_file_path, description=\"MSA alignments\"):\n",
        "    \"\"\"\n",
        "    Write alignment data to a3M format file.\n",
        "    \n",
        "    Args:\n",
        "        alignments_data: Either a list of alternating headers/sequences or a string containing alignments\n",
        "        output_file_path (str): Path for the output a3M file\n",
        "        description (str): Description for the file\n",
        "    \n",
        "    Returns:\n",
        "        str: Path to the created a3M file\n",
        "    \"\"\"\n",
        "    output_path = Path(output_file_path)\n",
        "    \n",
        "    # Handle both list and string input formats\n",
        "    if isinstance(alignments_data, list):\n",
        "        alignments_string = '\\n'.join(alignments_data)\n",
        "    elif isinstance(alignments_data, str):\n",
        "        alignments_string = alignments_data\n",
        "    else:\n",
        "        raise ValueError(\"alignments_data must be either a list or string\")\n",
        "    \n",
        "    # Count sequences for reporting\n",
        "    sequence_count = alignments_string.count('>')\n",
        "    \n",
        "    print(f\"Writing {sequence_count} sequences to a3M format: {output_path}\")\n",
        "    \n",
        "    try:\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            # Write the alignments\n",
        "            f.write(alignments_string)\n",
        "            \n",
        "            # Ensure file ends with newline\n",
        "            if not alignments_string.endswith('\\n'):\n",
        "                f.write('\\n')\n",
        "        \n",
        "        # Verify the file was created successfully\n",
        "        if output_path.exists():\n",
        "            file_size = output_path.stat().st_size\n",
        "            print(f\"Successfully created a3M file:\")\n",
        "            print(f\"File: {output_path}\")\n",
        "            print(f\"Size: {file_size:,} bytes\")\n",
        "            print(f\"Sequences: {sequence_count}\")\n",
        "            \n",
        "            return str(output_path)\n",
        "        else:\n",
        "            raise IOError(f\"Failed to create file {output_path}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error writing a3M file: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def process_msa_alignments(msa_response_dict, databases, sequence, max_sequences_per_db=10000, output_file=\"merged_alignments_protein.a3m\"):\n",
        "    \"\"\"\n",
        "    Process MSA alignments from multiple databases and merge them into A3M format.\n",
        "    \n",
        "    Args:\n",
        "        msa_response_dict (dict): MSA response data containing alignments\n",
        "        databases (list): List of database names to process\n",
        "        sequence (str): Query sequence for alignment\n",
        "        max_sequences_per_db (int): Maximum number of sequences to parse per database\n",
        "        output_file (str): Output A3M file path\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (merged_alignments_protein, a3m_file_path)\n",
        "            - merged_alignments_protein: List of merged alignments\n",
        "            - a3m_file_path: Path to the created A3M file\n",
        "    \"\"\"\n",
        "    all_parsed_dataset_output = []\n",
        "    \n",
        "    for num_done, database in enumerate(databases):\n",
        "        print(f\"Parsing results from database: {database}\")\n",
        "\n",
        "        # Pull string of alignments stored in json output for specific dataset\n",
        "        a3m_dict_msa_search = msa_response_dict['alignments'][database]['a3m']['alignment']\n",
        "\n",
        "        a3m_dict_msa_search_parsed = parse_sequences(a3m_dict_msa_search, max_sequences_per_db, sequence)\n",
        "\n",
        "        num_sequences_aligned = (len(a3m_dict_msa_search_parsed))\n",
        "        print(f\"Number of sequences aligned: {num_sequences_aligned}\")\n",
        "\n",
        "        all_parsed_dataset_output.extend(a3m_dict_msa_search_parsed)\n",
        "\n",
        "    # Sort all the alignments based off of the alignment score\n",
        "    all_parsed_dataset_output.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    # Now that the alignments across all datasets are sorted, reformat each entry to name and sequence\n",
        "    sorted_parsed_output_formatted = []\n",
        "    for align_tuple in all_parsed_dataset_output:\n",
        "        sorted_parsed_output_formatted.append(align_tuple[0])\n",
        "        sorted_parsed_output_formatted.append(align_tuple[1])\n",
        "\n",
        "    merged_alignments_protein = [f\">query_sequence\\n{sequence}\"]\n",
        "    merged_alignments_protein.extend(sorted_parsed_output_formatted)\n",
        "\n",
        "    print(f\"Total merged alignments: {len(merged_alignments_protein)}\")\n",
        "\n",
        "    # Write merged_alignments_protein to a3M format\n",
        "    a3m_file_path = write_alignments_to_a3m(\n",
        "        merged_alignments_protein, \n",
        "        output_file, \n",
        "        description=f\"Merged protein alignments from MSA-Search NIM ({', '.join(databases)})\"\n",
        "    )\n",
        "    \n",
        "    return merged_alignments_protein, a3m_file_path\n",
        "\n",
        "\n",
        "def write_filtered_a3m(alignments_data, output_file_path, max_sequences=None, min_length=None, description=\"Filtered MSA alignments\"):\n",
        "    \"\"\"\n",
        "    Write alignment data to a3M format with optional filtering.\n",
        "    \n",
        "    Args:\n",
        "        alignments_data: String containing alignments in FASTA-like format\n",
        "        output_file_path (str): Path for the output a3M file\n",
        "        max_sequences (int, optional): Maximum number of sequences to include\n",
        "        min_length (int, optional): Minimum sequence length (excluding gaps)\n",
        "        description (str): Description for the file\n",
        "    \n",
        "    Returns:\n",
        "        str: Path to the created a3M file\n",
        "    \"\"\"\n",
        "    output_path = Path(output_file_path)\n",
        "    \n",
        "    # Parse sequences from the input data\n",
        "    if isinstance(alignments_data, str):\n",
        "        lines = alignments_data.strip().split('\\n')\n",
        "    else:\n",
        "        lines = '\\n'.join(alignments_data).strip().split('\\n')\n",
        "    \n",
        "    sequences = []\n",
        "    current_header = None\n",
        "    current_sequence = \"\"\n",
        "    \n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith('>'):\n",
        "            # Save previous sequence if it exists\n",
        "            if current_header is not None:\n",
        "                sequences.append((current_header, current_sequence))\n",
        "            current_header = line\n",
        "            current_sequence = \"\"\n",
        "        else:\n",
        "            current_sequence += line\n",
        "    \n",
        "    # Don't forget the last sequence\n",
        "    if current_header is not None:\n",
        "        sequences.append((current_header, current_sequence))\n",
        "    \n",
        "    print(f\"Parsed {len(sequences)} sequences from input data\")\n",
        "    \n",
        "    # Apply filters\n",
        "    filtered_sequences = []\n",
        "    \n",
        "    for header, sequence in sequences:\n",
        "        # Apply minimum length filter (count non-gap characters)\n",
        "        if min_length is not None:\n",
        "            non_gap_length = len(sequence.replace('-', '').replace('.', ''))\n",
        "            if non_gap_length < min_length:\n",
        "                continue\n",
        "        \n",
        "        filtered_sequences.append((header, sequence))\n",
        "        \n",
        "        # Apply maximum sequences limit\n",
        "        if max_sequences is not None and len(filtered_sequences) >= max_sequences:\n",
        "            break\n",
        "    \n",
        "    print(f\"After filtering: {len(filtered_sequences)} sequences\")\n",
        "    if max_sequences:\n",
        "        print(f\"Max sequences limit: {max_sequences}\")\n",
        "    if min_length:\n",
        "        print(f\"Min length filter: {min_length}\")\n",
        "    \n",
        "    # Write to a3M format\n",
        "    try:\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            # Write sequences\n",
        "            for header, sequence in filtered_sequences:\n",
        "                f.write(f\"{header}\\n{sequence}\\n\")\n",
        "        \n",
        "        # Report success\n",
        "        file_size = output_path.stat().st_size\n",
        "        print(f\"Successfully created filtered a3M file:\")\n",
        "        print(f\"File: {output_path}\")\n",
        "        print(f\"Size: {file_size:,} bytes\")\n",
        "        print(f\"Sequences: {len(filtered_sequences)}\")\n",
        "        \n",
        "        return str(output_path)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error writing filtered a3M file: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def analyze_a3m_file(file_path):\n",
        "    \"\"\"\n",
        "    Analyze an a3M file and provide statistics.\n",
        "    \n",
        "    Args:\n",
        "        file_path (str): Path to the a3M file\n",
        "    \"\"\"\n",
        "    file_path = Path(file_path)\n",
        "    \n",
        "    if not file_path.exists():\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Analyzing a3M file: {file_path.name}\")\n",
        "    \n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        \n",
        "        # Count statistics\n",
        "        total_lines = len(lines)\n",
        "        comment_lines = sum(1 for line in lines if line.startswith('#'))\n",
        "        sequence_headers = sum(1 for line in lines if line.startswith('>'))\n",
        "        sequence_lines = total_lines - comment_lines - sequence_headers\n",
        "        \n",
        "        # Calculate sequence lengths\n",
        "        sequence_lengths = []\n",
        "        current_sequence = \"\"\n",
        "        \n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line.startswith('#'):\n",
        "                continue\n",
        "            elif line.startswith('>'):\n",
        "                if current_sequence:\n",
        "                    sequence_lengths.append(len(current_sequence))\n",
        "                current_sequence = \"\"\n",
        "            else:\n",
        "                current_sequence += line\n",
        "        \n",
        "        # Don't forget the last sequence\n",
        "        if current_sequence:\n",
        "            sequence_lengths.append(len(current_sequence))\n",
        "        \n",
        "        # File statistics\n",
        "        file_size = file_path.stat().st_size\n",
        "        \n",
        "        print(f\"File Statistics:\")\n",
        "        print(f\"File size: {file_size:,} bytes\")\n",
        "        print(f\"Total lines: {total_lines}\")\n",
        "        print(f\"Comment lines: {comment_lines}\")\n",
        "        print(f\"Sequence headers: {sequence_headers}\")\n",
        "        print(f\"Sequence lines: {sequence_lines}\")\n",
        "        \n",
        "        if sequence_lengths:\n",
        "            avg_length = sum(sequence_lengths) / len(sequence_lengths)\n",
        "            min_length = min(sequence_lengths)\n",
        "            max_length = max(sequence_lengths)\n",
        "            \n",
        "            print(f\"Sequence Statistics:\")\n",
        "            print(f\"Number of sequences: {len(sequence_lengths)}\")\n",
        "            print(f\"Average length: {avg_length:.1f}\")\n",
        "            print(f\"Length range: {min_length} - {max_length}\")\n",
        "            \n",
        "            # Show first sequence as example\n",
        "            with open(file_path, 'r') as f:\n",
        "                content = f.read()\n",
        "                \n",
        "            # Find first sequence\n",
        "            lines = content.split('\\n')\n",
        "            for i, line in enumerate(lines):\n",
        "                if line.startswith('>') and not line.startswith('#'):\n",
        "                    header = line\n",
        "                    sequence = \"\"\n",
        "                    j = i + 1\n",
        "                    while j < len(lines) and not lines[j].startswith('>'):\n",
        "                        if not lines[j].startswith('#'):\n",
        "                            sequence += lines[j].strip()\n",
        "                        j += 1\n",
        "                    \n",
        "                    print(f\"First sequence example:\")\n",
        "                    print(f\"Header: {header}\")\n",
        "                    print(f\"Length: {len(sequence)}\")\n",
        "                    print(f\"Preview: {sequence[:80]}{'...' if len(sequence) > 80 else ''}\")\n",
        "                    break\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing file: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parse the MSA alignment results to merge results from all datasets into a single `A3M` format file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_alignments_protein, a3m_file_path = process_msa_alignments(\n",
        "    msa_response_dict,\n",
        "    databases,\n",
        "    sequence,\n",
        "    max_sequences_per_db=10000,\n",
        "    output_file=\"merged_alignments_protein.a3m\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download the `A3M` file to your local machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "#files.download(os.path.join(output_dir, \"merged_alignments_protein.a3m\"))\n",
        "files.download(\"merged_alignments_protein.a3m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.5 Analyze the `A3M` Format File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze all created a3M files\n",
        "print(\"=\" * 60)\n",
        "print(\"A3M FILE ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "files_to_analyze = [\n",
        "    \"merged_alignments_protein.a3m\",\n",
        "]\n",
        "\n",
        "for file_name in files_to_analyze:\n",
        "    if Path(file_name).exists():\n",
        "        analyze_a3m_file(file_name)\n",
        "        print(\"-\" * 40)\n",
        "    else:\n",
        "        print(f\"File not found: {file_name}\")\n",
        "        print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.6 Visualize the MSA Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read in the MSA output\n",
        "with open('merged_alignments_protein.a3m', 'r') as file:\n",
        "    msa_output = file.read()\n",
        "    msa_output = msa_output.split('\\n')\n",
        "    seq_list = msa_output[1::2]\n",
        "    seq_names = msa_output[::2]\n",
        "    #remove '>' from seq_names\n",
        "    seq_names = [name.replace('>', '') for name in seq_names]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "BLUE = (120/255, 94/255, 240/255) # `indigo` from IBM colorblind friendly\n",
        "ORANGE = (254/255, 97/255, 0) # from IBM colorblind friendly\n",
        "GREEN = (118/255, 185/255, 0/255) \n",
        "\n",
        "\n",
        "def GetConsensus(msa):\n",
        "    \"\"\"Calculate per-index similarity of all strings compared to the baseline (first string).\"\"\"\n",
        "    if not msa or len(msa) < 2:\n",
        "        return []\n",
        "    \n",
        "    baseline = msa[0]\n",
        "    max_length = max(len(seq) for seq in msa)\n",
        "    consensus = []\n",
        "    \n",
        "    # Analyze each position\n",
        "    for pos in range(max_length):\n",
        "        baseline_char = baseline[pos] if pos < len(baseline) else '-'\n",
        "        \n",
        "        # Count matches with baseline at this position\n",
        "        matches = 0\n",
        "        total_strings = 0\n",
        "        \n",
        "        for seq in msa[1:]:\n",
        "            if pos < len(seq):\n",
        "                seq_char = seq[pos]\n",
        "                if seq_char != '-' and baseline_char != '-':\n",
        "                    if seq_char == baseline_char:\n",
        "                        matches += 1\n",
        "                    total_strings += 1\n",
        "        \n",
        "        # Calculate similarity percentage for this position\n",
        "        similarity = (matches / total_strings * 100) if total_strings > 0 else 0\n",
        "        consensus.append({\n",
        "            'position': pos,\n",
        "            'baseline_char': baseline_char,\n",
        "            'similarity_percent': similarity,\n",
        "            'matches': matches,\n",
        "            'total_comparisons': total_strings\n",
        "        })\n",
        "    \n",
        "    return consensus\n",
        "    \n",
        "\n",
        "def ConsensusHistogram(msa, \n",
        "                       color=GREEN, \n",
        "                       start=None, end=None, \n",
        "                       tick_fontsize=8, \n",
        "                       label_fontsize=12, \n",
        "                       title_fontsize=14,\n",
        "                       ):\n",
        "    \"\"\"Create histogram of consensus frequencies using seaborn with customizable fonts.\"\"\"\n",
        "    if not msa:\n",
        "        return None\n",
        "    \n",
        "    # Slice MSA if start/end specified\n",
        "    if start is not None or end is not None:\n",
        "        msa = [seq[start:end] for seq in msa]\n",
        "    \n",
        "    # Transpose to get columns (positions)\n",
        "    positions = list(zip(*msa))\n",
        "    \n",
        "    # Calculate consensus frequency at each position\n",
        "    consensus_results = GetConsensus(msa)\n",
        "    consensus_freqs = [item['similarity_percent'] for item in consensus_results]\n",
        "    \n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(16, 4))\n",
        "    \n",
        "    # Use seaborn for better styling\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    \n",
        "    # Create the histogram\n",
        "    ax = sns.barplot(x=list(range(len(consensus_freqs))), \n",
        "                     y=consensus_freqs, \n",
        "                     color=color, \n",
        "                     alpha=0.9,\n",
        "                     edgecolor='white',\n",
        "                     )\n",
        "    \n",
        "    # Remove space between bars\n",
        "    for patch in ax.patches:\n",
        "        current_width = patch.get_width()\n",
        "        new_width = 1.0  # Set width to 1.0 to eliminate gaps\n",
        "        diff = current_width - new_width\n",
        "        patch.set_width(new_width)\n",
        "        patch.set_x(patch.get_x() + diff * .5) # Adjust x-position to center the bar\n",
        "\n",
        "\n",
        "    # Automatically set major x-axis ticks based on figure size\n",
        "    num_positions = len(consensus_freqs)\n",
        "    if num_positions > 0:\n",
        "        # Calculate optimal number of ticks based on figure width\n",
        "        # Aim for roughly 8-12 ticks across the figure\n",
        "        optimal_ticks = min(12, max(5, num_positions // 10))\n",
        "        \n",
        "        # Calculate tick positions\n",
        "        tick_positions = np.linspace(1, num_positions, optimal_ticks, dtype=int)\n",
        "        \n",
        "        # Set x-axis ticks and labels\n",
        "        ax.set_xticks(tick_positions)\n",
        "        ax.set_xticklabels([f'{pos}' for pos in tick_positions], fontsize=tick_fontsize)\n",
        "    \n",
        "    # Customize tick marks\n",
        "    ax.tick_params(axis='both', labelsize=tick_fontsize)\n",
        "    \n",
        "    # Customize labels with specified font sizes\n",
        "    ax.set_xlabel('Sequence Position', fontsize=label_fontsize, fontweight='bold')\n",
        "    ax.set_ylabel('Consensus Frequency', fontsize=label_fontsize, fontweight='bold')\n",
        "    ax.set_title('Consensus Frequency by Position Across All MSA Sequences', fontsize=title_fontsize, fontweight='bold')\n",
        "    \n",
        "    # Add grid with custom styling\n",
        "    ax.grid(True, alpha=0.3, linestyle='--')\n",
        "    \n",
        "    # Adjust layout\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ConsensusHistogram(seq_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
