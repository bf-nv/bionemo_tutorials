{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7PzNycELiTF"
      },
      "source": [
        "# Boltz-2 NIM for Protein/Ligand Co-Folding and Affinity Prediction, including MSA-Search NIM for Alignments\n",
        "\n",
        "## Demo to run in Google Colab environment\n",
        "\n",
        "[MSA-Search](https://docs.nvidia.com/nim/bionemo/msa-search/latest/overview.html) Multiple Sequence Alignment (MSA) compares a query amino acid sequence to protein databases, aligning similar sequences to identify conserved regions despite differences in length or motifs. The resulting alignments enhance structural prediction models like AlphaFold2 and OpenFold by leveraging the structural similarity of homologous sequences.\n",
        "\n",
        "[Boltz-2](https://docs.nvidia.com/nim/bionemo/boltz2/latest/index.html) Boltz-2 NIM delivers advanced biomolecular structure and binding affinity predictions for proteins, RNA, DNA, and other molecules. Built on the Boltz-2 architecture, it enables accurate modeling of complex structures and quantifies molecular interactions across diverse configurations.\n",
        "\n",
        "04Sept2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf1FvaW7LiTG"
      },
      "source": [
        "  ## 1.1 Set Up the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzVnNGuQLiTG",
        "outputId": "fb3fec96-81b4-4eaa-f925-1a50e84bff47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (0.28.1)\n",
            "Requirement already satisfied: fastapi[standard] in /usr/local/lib/python3.12/dist-packages (0.116.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx) (0.16.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]) (0.47.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]) (4.15.0)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.8 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (0.0.10)\n",
            "Requirement already satisfied: jinja2>=3.1.5 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]) (3.1.6)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]) (0.0.20)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]) (2.3.0)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (0.35.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from email-validator>=2.0.0->fastapi[standard]) (2.7.0)\n",
            "Requirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (0.17.3)\n",
            "Requirement already satisfied: rich-toolkit>=0.14.8 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (0.15.1)\n",
            "Requirement already satisfied: fastapi-cloud-cli>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (0.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=3.1.5->fastapi[standard]) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi[standard]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi[standard]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi[standard]) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx) (1.3.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.12.0->uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (8.2.1)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (1.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (6.0.2)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (15.0.1)\n",
            "Requirement already satisfied: rignore>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (0.6.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (2.35.2)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (13.9.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (2.19.2)\n",
            "Requirement already satisfied: urllib3>=1.26.11 in /usr/local/lib/python3.12/dist-packages (from sentry-sdk>=2.20.0->fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib numpy pandas seaborn scikit-learn tqdm httpx \"fastapi[standard]\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "68DEY69bLiTG"
      },
      "outputs": [],
      "source": [
        "import os, requests, re\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Optional, Tuple, List\n",
        "from time import perf_counter\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "import asyncio\n",
        "import logging\n",
        "import sys\n",
        "from google.colab import userdata, files\n",
        "\n",
        "# Check for required dependencies\n",
        "missing_deps = []\n",
        "try:\n",
        "    import httpx\n",
        "except ImportError:\n",
        "    missing_deps.append(\"httpx\")\n",
        "try:\n",
        "    from fastapi import HTTPException\n",
        "except ImportError:\n",
        "    missing_deps.append(\"fastapi\")\n",
        "\n",
        "if missing_deps:\n",
        "    print(\"Error: Missing required dependencies. Please install them using:\")\n",
        "    print(f\"pip install {' '.join(missing_deps)}\")\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjjC4slaLiTH"
      },
      "source": [
        "  ### Define Input File with SMILES data and Output Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "BcGbdgy_LiTH"
      },
      "outputs": [],
      "source": [
        "# edit to match your dataset path. Ensure you use the `raw` URL path if the file is hosted on GitHub.\n",
        "\n",
        "CSV_FILE = \"https://raw.githubusercontent.com/bf-nv/bionemo_tutorials/refs/heads/main/RORc_SMILES_and_pIC50.csv\"\n",
        "\n",
        "OUTPUT_DIR = \"/content/output\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5ExvVNaLiTH"
      },
      "source": [
        "  ## 1.2 Set Up `output` Directory and `API_KEY`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "qshwJaGbLiTH"
      },
      "outputs": [],
      "source": [
        "API_KEY = userdata.get('API_KEY')\n",
        "\n",
        "# Ensure output directory exists, create if not present\n",
        "if os.path.exists(OUTPUT_DIR):\n",
        "    shutil.rmtree(OUTPUT_DIR)\n",
        "    os.makedirs(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0yGxZVcLiTH"
      },
      "source": [
        "  ## 1.3 Define `MSA-Search` Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "CXj7k0ZmLiTH"
      },
      "outputs": [],
      "source": [
        "MSA_DATABASES = ['Uniref30_2302', 'colabfold_envdb_202108', 'PDB70_220313']\n",
        "\n",
        "def msa_search(sequence, API_KEY, databases=MSA_DATABASES):\n",
        "    msa_search_url = \"https://health.api.nvidia.com/v1/biology/colabfold/msa-search/predict\"\n",
        "    payload = {\n",
        "        \"sequence\": sequence,\n",
        "        \"databases\": databases,\n",
        "        \"e_value\": 0.0001,\n",
        "        \"iterations\": 1,\n",
        "        \"max_msa_sequences\": 10000,\n",
        "        \"run_structural_template_search\": False,\n",
        "        \"output_alignment_formats\": [\"a3m\"],\n",
        "    }\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        \"content-type\": \"application/json\",\n",
        "        \"NVCF-POLL-SECONDS\": \"300\",\n",
        "    }\n",
        "    # Call MSA-Search NIM\n",
        "    response = requests.post(msa_search_url, json=payload, headers=headers)\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "def parse_sequences(input_string, n, sequence):\n",
        "    \"\"\"\n",
        "    Parse the output of alignments from the MSA-Search NIM to be used downstream\n",
        "\n",
        "    Args:\n",
        "        input_string (str): The output file of alignments in a string format\n",
        "        n (int): The amount of alignments to return from the output when parsing\n",
        "        sequence (str): The query sequence for alignment\n",
        "\n",
        "    Returns:\n",
        "        list: A list of alignment identifiers and sequences, starting with the query,\n",
        "              where the amount of sequences is given by n\n",
        "    \"\"\"\n",
        "    # Output is parsed to have a line for the sequence id and sequence itself so `n` returns correlates to n*2 lines\n",
        "    n = n * 2\n",
        "    # First, handle the `Query` block separately\n",
        "    lines = input_string.strip().split('\\n')\n",
        "    # Now process the rest of the lines\n",
        "    remaining_string = \"\\n\".join(lines[:])\n",
        "    # Regex to find blocks starting with `>` and then followed by a sequence.\n",
        "    pattern = re.compile(r'\\n>(.*?)\\n(.*?)(?=\\n>|\\Z)', re.DOTALL)\n",
        "    matches = pattern.finditer(remaining_string)\n",
        "    output_list_to_order = []\n",
        "    for match in matches:\n",
        "        # The name is the first capturing group, split by tab and take the first part\n",
        "        name_full = match.group(1).split('\\t')[0]\n",
        "        SW_score = match.group(1).split('\\t')[1]\n",
        "        # The sequence is the second capturing group\n",
        "        sequence_raw = match.group(2).strip()\n",
        "        aligned_sequence = ''.join(char for char in sequence_raw if char.isupper() or not char.isalpha())\n",
        "        # Store the aligned sequence in the list of outputs by name, sequence, Smith-Waterman score\n",
        "        output_list_to_order.append((f'>{name_full}', aligned_sequence, int(SW_score)))\n",
        "    output_lines = output_list_to_order[:n]\n",
        "    return output_lines\n",
        "\n",
        "\n",
        "def validate_a3m_format(alignments_string):\n",
        "    \"\"\"\n",
        "    Validate that the alignment string follows A3M format.\n",
        "\n",
        "    Args:\n",
        "        alignments_string (str): String containing alignments\n",
        "\n",
        "    Returns:\n",
        "        bool: True if valid A3M format, False otherwise\n",
        "    \"\"\"\n",
        "    lines = alignments_string.strip().split('\\n')\n",
        "    if len(lines) < 2:\n",
        "        return False\n",
        "\n",
        "    # Check that we have alternating header and sequence lines\n",
        "    for i, line in enumerate(lines):\n",
        "        if i % 2 == 0:  # Even indices should be headers\n",
        "            if not line.startswith('>'):\n",
        "                return False\n",
        "        else:  # Odd indices should be sequences\n",
        "            if line.startswith('>'):\n",
        "                return False\n",
        "            # Sequences should only contain valid amino acid characters and gaps\n",
        "            if not all(c in 'ACDEFGHIKLMNPQRSTVWY-' for c in line.upper()):\n",
        "                return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def write_alignments_to_a3m(alignments_data, uniprot_id, output_dir):\n",
        "    \"\"\"\n",
        "    Write alignment data to a3M format file.\n",
        "\n",
        "    Args:\n",
        "        alignments_data: Either a list of alternating headers/sequences or a string containing alignments\n",
        "        uniprot_id (str): Uniprot ID of the protein\n",
        "        output_dir (str): Directory for the output a3M file\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the created a3M file\n",
        "    \"\"\"\n",
        "    # Ensure output directory exists\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    output_path = Path(output_dir) / f\"{uniprot_id}_msa_alignments.a3m\"\n",
        "\n",
        "    # Handle both list and string input formats\n",
        "    if isinstance(alignments_data, list):\n",
        "        alignments_string = '\\n'.join(alignments_data)\n",
        "    elif isinstance(alignments_data, str):\n",
        "        alignments_string = alignments_data\n",
        "    else:\n",
        "        raise ValueError(\"alignments_data must be either a list or string\")\n",
        "\n",
        "    # Validate A3M format\n",
        "    if not alignments_string.strip():\n",
        "        raise ValueError(\"Empty alignment data provided\")\n",
        "\n",
        "    # Count sequences for reporting\n",
        "    sequence_count = alignments_string.count('>')\n",
        "    if sequence_count == 0:\n",
        "        raise ValueError(\"No sequences found in alignment data\")\n",
        "\n",
        "    # Validate A3M format structure\n",
        "    if not validate_a3m_format(alignments_string):\n",
        "        print(\"Warning: Alignment data may not follow strict A3M format\")\n",
        "        print(\"Proceeding with file creation...\")\n",
        "\n",
        "    print(f\"Writing {sequence_count} sequences to A3M format: {output_path}\")\n",
        "\n",
        "    try:\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            # Write the alignments\n",
        "            f.write(alignments_string)\n",
        "            # Ensure file ends with newline\n",
        "            if not alignments_string.endswith('\\n'):\n",
        "                f.write('\\n')\n",
        "\n",
        "        # Verify the file was created successfully\n",
        "        if output_path.exists():\n",
        "            file_size = output_path.stat().st_size\n",
        "            print(f\"Successfully created A3M file:\")\n",
        "            print(f\"File: {output_path}\")\n",
        "            print(f\"Size: {file_size:,} bytes\")\n",
        "            print(f\"Sequences: {sequence_count}\")\n",
        "\n",
        "            # Download the file to the user's machine\n",
        "            try:\n",
        "                files.download(str(output_path))\n",
        "                print(f\"File downloaded successfully: {output_path}\")\n",
        "            except Exception as download_error:\n",
        "                print(f\"Warning: Could not download file automatically: {download_error}\")\n",
        "                print(f\"File is available at: {output_path}\")\n",
        "\n",
        "            return str(output_path)\n",
        "        else:\n",
        "            raise IOError(f\"Failed to create file {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing A3M file: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def process_msa_alignments(msa_response_dict, sequence, uniprot_id, output_dir, databases=MSA_DATABASES, max_sequences_per_db=10000):\n",
        "    \"\"\"\n",
        "    Process MSA alignments from multiple databases and merge them into A3M format.\n",
        "\n",
        "    Args:\n",
        "        msa_response_dict (dict): MSA response data containing alignments\n",
        "        sequence (str): Query sequence for alignment\n",
        "        uniprot_id (str): Uniprot ID of the protein\n",
        "        output_dir (str): Output directory for the A3M file\n",
        "        databases (list): List of database names to process\n",
        "        max_sequences_per_db (int): Maximum number of sequences to parse per database\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the created A3M file\n",
        "    \"\"\"\n",
        "    all_parsed_dataset_output = []\n",
        "    for database in databases:\n",
        "        print(f\"Parsing results from database: {database}\")\n",
        "        # Pull string of alignments stored in json output for specific dataset\n",
        "        a3m_dict_msa_search = msa_response_dict['alignments'][database]['a3m']['alignment']\n",
        "        a3m_dict_msa_search_parsed = parse_sequences(a3m_dict_msa_search, max_sequences_per_db, sequence)\n",
        "        num_sequences_aligned = (len(a3m_dict_msa_search_parsed))\n",
        "        print(f\"Number of sequences aligned: {num_sequences_aligned}\")\n",
        "        all_parsed_dataset_output.extend(a3m_dict_msa_search_parsed)\n",
        "    # Sort all the alignments based off of the alignment score\n",
        "    all_parsed_dataset_output.sort(key=lambda x: x[2], reverse=True)\n",
        "    # Now that the alignments across all datasets are sorted, reformat each entry to name and sequence\n",
        "    sorted_parsed_output_formatted = []\n",
        "    for align_tuple in all_parsed_dataset_output:\n",
        "        sorted_parsed_output_formatted.append(align_tuple[0])\n",
        "        sorted_parsed_output_formatted.append(align_tuple[1])\n",
        "    merged_alignments_protein = [f\">query_sequence\\n{sequence}\"]\n",
        "    merged_alignments_protein.extend(sorted_parsed_output_formatted)\n",
        "    print(f\"Total merged alignments: {len(merged_alignments_protein)}\")\n",
        "    # Write merged_alignments_protein to a3M format\n",
        "    a3m_file_path = write_alignments_to_a3m(\n",
        "        merged_alignments_protein,\n",
        "        uniprot_id,\n",
        "        output_dir\n",
        "    )\n",
        "    return a3m_file_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkuGXjY9LiTI"
      },
      "source": [
        "  ## 1.4 Run `MSA-Search` and save `A3M` alignment file output to local directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMZ16yQ9LiTI"
      },
      "source": [
        "  ### Provide Sequence Information\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  **NOTE:** Ensure the sequence is a string and does not contain any whitespace, special characters, nor carriage returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "5YSlQRMULiTI"
      },
      "outputs": [],
      "source": [
        "# Example sequence using human RORc from PDB:4wqp_A\n",
        "# http://rcsb.org/structure/4WQP\n",
        "\n",
        "# >4WQP_1|Chain A|Nuclear receptor ROR-gamma|Homo sapiens (9606)\n",
        "uniprot_id = \"4wqp_1\"\n",
        "sequence = \"MHHHHHHGENLYFQGSAPYASLTEIEHLVQSVCKSYRETCQLRLEDLLRQRSNIFSREEVTGYQRKSMWEMWERCAHHLTEAIQYVVEFAKRLSGFMELCQNDQIVLLKAGAMEVVLVRMCRAYNADNRTVFFEGKYGGMELFRALGCSELISSIFDFSHSLSALHFSEDEIALYTALVLINAHRPGLQEKRKVEQLQYNLELAFHHHLCKTHRQSILAKLPPKGKLRSLCSQHVERLQIFQHLHPIVVQAAFPPLYKELFSGNS\"\n",
        "sequences = [(uniprot_id, sequence)]\n",
        "\n",
        "# NOTE: Ensure the sequence is a string and does not contain any whitespace, special characters, nor carriage returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "3_Y05C_qLiTI",
        "outputId": "7fe78a6e-b71b-4a3c-9e2c-90b2796e1c03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing protein: 4wqp_1\n",
            "Sequence length: 265\n",
            "Parsing results from database: Uniref30_2302\n",
            "Number of sequences aligned: 100\n",
            "Parsing results from database: colabfold_envdb_202108\n",
            "Number of sequences aligned: 100\n",
            "Parsing results from database: PDB70_220313\n",
            "Number of sequences aligned: 88\n",
            "Total merged alignments: 577\n",
            "Warning: Alignment data may not follow strict A3M format\n",
            "Proceeding with file creation...\n",
            "Writing 289 sequences to A3M format: /content/output/4wqp_1_msa_alignments.a3m\n",
            "Successfully created A3M file:\n",
            "File: /content/output/4wqp_1_msa_alignments.a3m\n",
            "Size: 81,161 bytes\n",
            "Sequences: 289\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e7be7836-0532-47f4-87fb-774c323b6cfe\", \"4wqp_1_msa_alignments.a3m\", 81161)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:40<00:00, 40.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully: /content/output/4wqp_1_msa_alignments.a3m\n",
            "Successfully processed 4wqp_1 -> /content/output/4wqp_1_msa_alignments.a3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for seq_id, seq in tqdm(sequences):\n",
        "    try:\n",
        "        print(f\"\\nProcessing protein: {seq_id}\")\n",
        "        print(f\"Sequence length: {len(seq)}\")\n",
        "\n",
        "        # Call MSA-Search NIM\n",
        "        msa_response_dict = msa_search(seq, API_KEY)\n",
        "\n",
        "        # Check if the response contains the expected data\n",
        "        if 'alignments' not in msa_response_dict:\n",
        "            print(f\"Warning: No alignments found for {seq_id}\")\n",
        "            continue\n",
        "\n",
        "        # Process and create A3M file\n",
        "        a3m_file_path = process_msa_alignments(msa_response_dict, seq, seq_id, OUTPUT_DIR)\n",
        "        print(f\"Successfully processed {seq_id} -> {a3m_file_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {seq_id}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a09JHLu2LiTI"
      },
      "source": [
        "  ## 1.5 List all created `A3M` alignment files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE06PJJfLiTJ",
        "outputId": "ac1190b7-b149-4125-ff74-87fd9a4ce7cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 1 A3M files:\n",
            "  - 4wqp_1_msa_alignments.a3m (81,161 bytes)\n",
            "\n",
            "All A3M files are available in: /content/output\n",
            "Files have been automatically downloaded to your machine.\n"
          ]
        }
      ],
      "source": [
        "# List all created A3M files\n",
        "import glob\n",
        "a3m_files = glob.glob(f\"{OUTPUT_DIR}/*.a3m\")\n",
        "a3m_files = sorted(a3m_files)\n",
        "print(f\"Created {len(a3m_files)} A3M files:\")\n",
        "for file_path in a3m_files:\n",
        "    file_size = Path(file_path).stat().st_size\n",
        "    print(f\"  - {Path(file_path).name} ({file_size:,} bytes)\")\n",
        "\n",
        "print(f\"\\nAll A3M files are available in: {OUTPUT_DIR}\")\n",
        "print(\"Files have been automatically downloaded to your machine.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-QSW-72LiTJ"
      },
      "source": [
        "  ## 1.6 If needed, trigger download of all `A3M` alignment files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Bt3Y1GaCLiTJ",
        "outputId": "7c072211-c452-4c28-9f01-70aa18447e3a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b77fa198-78be-4ed5-8338-81c147d95ab3\", \"4wqp_1_msa_alignments.a3m\", 81161)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "for file_path in a3m_files:\n",
        "    files.download(file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0s2HIgKLiTJ"
      },
      "source": [
        "  ## 2.1 Set-up `Boltz-2` Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6XvARDpLiTJ"
      },
      "source": [
        "  ### Configuration Constraints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "K20zslExLiTJ"
      },
      "outputs": [],
      "source": [
        "# MSA configuration\n",
        "MSA_DATA_DIR = OUTPUT_DIR\n",
        "MSA_FILE_NAME = f\"{seq_id}_msa_alignments.a3m\"\n",
        "MSA_FILE_PATH = Path(OUTPUT_DIR) / MSA_FILE_NAME\n",
        "if MSA_FILE_PATH.exists():\n",
        "    MSA_STATUS = True\n",
        "else:\n",
        "    MSA_STATUS = False\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "STATUS_URL = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/status/{task_id}\"\n",
        "BOLTZ2_BASE_URL = \"https://health.api.nvidia.com\"\n",
        "BOLTZ2_ENDPOINT = \"https://health.api.nvidia.com/v1/biology/mit/boltz2/predict\"\n",
        "REQUEST_TIMEOUT = 300  # 5 minutes\n",
        "NVCF_POLL_SECONDS = 300\n",
        "MANUAL_TIMEOUT_SECONDS = 400\n",
        "\n",
        "# Boltz2 prediction parameters\n",
        "BOLTZ2_CONFIG = {\n",
        "    \"recycling_steps\": 3,\n",
        "    \"sampling_steps\": 20,\n",
        "    \"diffusion_samples\": 1,\n",
        "    \"step_scale\": 1.64,\n",
        "    \"without_potentials\": True\n",
        "}\n",
        "\n",
        "# Required output columns for protein data\n",
        "REQUIRED_OUTPUT_COLUMNS = {\n",
        "    'smiles', 'uniprot_id', 'fasta_uniprot_seq', 'pic50',\n",
        "    'boltz2_plddt', 'boltz2_pic50', 'boltz2_pic50_conf',\n",
        "    'boltz2_msa', 'boltz2_runtime'\n",
        "}\n",
        "\n",
        "# CSV timestamp format\n",
        "CSV_TIMESTAMP_FORMAT = \"%Y_%m_%d\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GtwJQX2LiTJ"
      },
      "source": [
        "  ## 2.2 Use `csv_file` as Source of SMILES and pIC50 Data for Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7hyWRhlLiTJ",
        "outputId": "4a83b690-8601-4032-87c7-3ac9b2f5c736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded dataset: (22, 3)\n",
            "Columns: ['smiles', 'actual_pIC50', 'reference']\n",
            "\n",
            "First few rows:\n",
            "                                              smiles  actual_pIC50  \\\n",
            "0  CC(=O)N3CCN(c2ccc(CC(CC(C)C)S(=O)(=O)Cc1ccccc1...          7.43   \n",
            "1  CC(=O)N4CCN(c3ccc(CC(C1CCC1)S(=O)(=O)Cc2ccccc2...          7.96   \n",
            "2  CC(C)CN(Cc1ccc(s1)-c1ccc(N)nc1)S(=O)(=O)Cc1ccccc1          5.82   \n",
            "3  CC(C)CN(Cc2ccc(c1ccc(=O)[nH]c1)s2)S(=O)(=O)Cc3...          5.44   \n",
            "4  CC(C)CN(Cc2ccc(c1ccc(C(N)=O)cc1)cc2)S(=O)(=O)C...          7.48   \n",
            "\n",
            "         reference  \n",
            "0  BMCL 2014, 3891  \n",
            "1   JMC 2015, 5308  \n",
            "2  BMCL 2014, 2182  \n",
            "3  BMCL 2014, 2182  \n",
            "4  BMCL 2014, 3891  \n"
          ]
        }
      ],
      "source": [
        "# Check if file exists before loading\n",
        "if not CSV_FILE:\n",
        "    raise FileNotFoundError(f\"CSV file not found: {CSV_FILE}\")\n",
        "\n",
        "# Load the dataset\n",
        "try:\n",
        "    df = pd.read_csv(CSV_FILE, low_memory=False)\n",
        "    print(f\"Successfully loaded dataset: {df.shape}\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    print(df.head())\n",
        "except Exception as e:\n",
        "    raise IOError(f\"Error loading CSV file {CSV_FILE}: {e}\")\n",
        "\n",
        "# Validate that the dataset has the expected structure\n",
        "if df.empty:\n",
        "    raise ValueError(\"Dataset is empty\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuFslAPMLiTJ"
      },
      "source": [
        "  ## 2.3 Define `Boltz-2` Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA9ykXmiLiTJ",
        "outputId": "3b21cfa7-e168-4884-9c2f-40e948a8867a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <coroutine object run_queries at 0x126fcc30>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <lambda>\n",
            "KeyError: '__import__'\n",
            "Exception ignored in: <coroutine object run_queries at 0x126fcc30>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <lambda>\n",
            "KeyError: '__import__'\n"
          ]
        }
      ],
      "source": [
        "def boltz2_nim_query(\n",
        "    input_data: Dict[str, Any],\n",
        "    url: str = BOLTZ2_ENDPOINT\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Query the Boltz2 NIM with input data.\n",
        "\n",
        "    Args:\n",
        "        input_data: Dictionary containing the prediction request data\n",
        "        base_url: Base URL of the NIM service (default: from config)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing the prediction response\n",
        "\n",
        "    Raises:\n",
        "        requests.exceptions.RequestException: If the HTTP request fails\n",
        "    \"\"\"\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            url,\n",
        "            json=input_data,\n",
        "            headers=headers,\n",
        "            timeout=REQUEST_TIMEOUT\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "    except requests.exceptions.Timeout:\n",
        "        raise requests.exceptions.RequestException(f\"Request timed out after {REQUEST_TIMEOUT} seconds\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        error_msg = f\"Error querying NIM: {e}\"\n",
        "        if hasattr(e, 'response') and e.response is not None:\n",
        "            error_msg += f\"\\nResponse status: {e.response.status_code}\"\n",
        "            if hasattr(e.response, 'text'):\n",
        "                error_msg += f\"\\nResponse text: {e.response.text}\"\n",
        "        raise requests.exceptions.RequestException(error_msg)\n",
        "\n",
        "\n",
        "def create_payload(seq: str, smile_: str, msa_content: str = None) -> Dict[str, Any]:\n",
        "    \"\"\"Create the payload for Boltz2 NIM query.\n",
        "\n",
        "    Args:\n",
        "        seq (str): Protein sequence\n",
        "        smile_: SMILES string for the ligand\n",
        "        msa_content (str, optional): MSA alignment content as string\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: Payload for Boltz2 NIM query\n",
        "    \"\"\"\n",
        "    polymer_data = {\n",
        "        \"id\": \"A\",\n",
        "        \"molecule_type\": \"protein\",\n",
        "        \"sequence\": seq,\n",
        "    }\n",
        "\n",
        "    # Add MSA data if content is provided\n",
        "    if msa_content:\n",
        "        polymer_data[\"msa\"] = {\n",
        "            \"uniref90\": {\n",
        "                \"a3m\": {\n",
        "                    \"alignment\": msa_content,\n",
        "                    \"format\": \"a3m\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    # Construct complete payload using configuration constants\n",
        "    payload = {\n",
        "        \"polymers\": [polymer_data],\n",
        "        \"ligands\": [{\n",
        "            \"smiles\": smile_,\n",
        "            \"id\": \"L1\",\n",
        "            \"predict_affinity\": True\n",
        "        }],\n",
        "        **BOLTZ2_CONFIG\n",
        "    }\n",
        "    return payload\n",
        "\n",
        "\n",
        "def append_default_values(plddt_list, pic50_list, pic50_conf_list, msa_list, time_list, msa_status, time_val=None):\n",
        "    \"\"\"Append default values to all result lists while preserving ground truth data.\n",
        "\n",
        "    Note: This function is used to maintain the correct length of Boltz2 prediction result lists.\n",
        "    Ground truth values are preserved separately in the original DataFrame and protein CSV files.\n",
        "\n",
        "    Args:\n",
        "        plddt_list: List to append pLDDT values to\n",
        "        pic50_list: List to append pIC50 values to\n",
        "        pic50_conf_list: List to append pIC50 confidence values to\n",
        "        msa_list: List to append MSA status to\n",
        "        time_list: List to append timing values to\n",
        "        msa_status: Whether MSA was used\n",
        "        time_val: Runtime value to append (optional)\n",
        "    \"\"\"\n",
        "    # Append default/null values to maintain list consistency\n",
        "    plddt_list.append(None)\n",
        "    pic50_list.append(None)\n",
        "    pic50_conf_list.append(None)\n",
        "    msa_list.append(msa_status)\n",
        "    time_list.append(time_val)\n",
        "\n",
        "\n",
        "def process_msa_file(msa_file_path: Path = None) -> str:\n",
        "    \"\"\"\n",
        "    Process MSA file and return its content.\n",
        "\n",
        "    Args:\n",
        "        msa_file_path: Path to MSA file (default: global MSA_FILE_PATH)\n",
        "\n",
        "    Returns:\n",
        "        MSA alignment data as string\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If MSA file doesn't exist\n",
        "        IOError: If there's an error reading the file\n",
        "    \"\"\"\n",
        "    if msa_file_path is None:\n",
        "        msa_file_path = MSA_FILE_PATH\n",
        "\n",
        "    if not msa_file_path.exists():\n",
        "        raise FileNotFoundError(f\"MSA file not found: {msa_file_path}\")\n",
        "    try:\n",
        "        with open(msa_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read().strip()\n",
        "            if not content:\n",
        "                raise IOError(f\"MSA file is empty: {msa_file_path}\")\n",
        "            return content\n",
        "    except UnicodeDecodeError as e:\n",
        "        raise IOError(f\"Error decoding MSA file {msa_file_path}: {e}\")\n",
        "    except Exception as e:\n",
        "        raise IOError(f\"Error reading MSA file {msa_file_path}: {e}\")\n",
        "\n",
        "\n",
        "def validate_response(result: Dict[str, Any], seq_id: str) -> Tuple[Optional[float], List, List]:\n",
        "    \"\"\"\n",
        "    Validate API response and extract required data.\n",
        "\n",
        "    Args:\n",
        "        result: API response dictionary\n",
        "        seq_id: Protein sequence identifier for error reporting\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (plddt_score, pic50_values, pic50_confidence_values)\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If required data is missing from response\n",
        "    \"\"\"\n",
        "    # Validate affinities data\n",
        "    if 'affinities' not in result:\n",
        "        raise ValueError(f\"Missing 'affinities' key in response for {seq_id}\")\n",
        "    if 'L1' not in result['affinities']:\n",
        "        raise ValueError(f\"Missing 'L1' ligand data in affinities for {seq_id}\")\n",
        "    # Extract pLDDT scores\n",
        "    if 'complex_plddt_scores' not in result or not result['complex_plddt_scores']:\n",
        "        print(f\"Warning: Missing or empty pLDDT scores for {seq_id}\")\n",
        "        plddt_indiv = None\n",
        "    else:\n",
        "        try:\n",
        "            plddt_indiv = float(np.mean(result['complex_plddt_scores']))\n",
        "        except (TypeError, ValueError) as e:\n",
        "            print(f\"Warning: Error calculating pLDDT mean for {seq_id}: {e}\")\n",
        "            plddt_indiv = None\n",
        "    # Extract affinity data\n",
        "    try:\n",
        "        pic50_indiv = result['affinities']['L1']['affinity_pic50']\n",
        "        pic50_conf_indiv = result['affinities']['L1']['affinity_probability_binary']\n",
        "        # Validate that we got lists\n",
        "        if not isinstance(pic50_indiv, list) or not isinstance(pic50_conf_indiv, list):\n",
        "            raise ValueError(f\"Expected lists for affinity data, got {type(pic50_indiv)} and {type(pic50_conf_indiv)}\")\n",
        "    except KeyError as e:\n",
        "        raise ValueError(f\"Missing required affinity data key {e} for {seq_id}\")\n",
        "    return plddt_indiv, pic50_indiv, pic50_conf_indiv\n",
        "\n",
        "\n",
        "def save_protein_results_to_csv(protein_data: list, protein_id: str, output_dir: str = None, msa_status: bool = None):\n",
        "    \"\"\"\n",
        "    Save results for a single protein to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        protein_data: List of dictionaries containing ligand data\n",
        "        protein_id: Identifier for the protein\n",
        "        output_dir: Directory to save the CSV file (default: global OUTPUT_DIR)\n",
        "        msa_status: Whether MSA was used for predictions (default: global MSA_STATUS)\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If protein_data is empty or invalid\n",
        "        IOError: If there's an error writing the file\n",
        "    \"\"\"\n",
        "    if not protein_data:\n",
        "        return\n",
        "    if not isinstance(protein_data, list) or not all(isinstance(item, dict) for item in protein_data):\n",
        "        raise ValueError(\"protein_data must be a list of dictionaries\")\n",
        "\n",
        "    # Use global constants if not provided\n",
        "    if output_dir is None:\n",
        "        output_dir = OUTPUT_DIR\n",
        "    if msa_status is None:\n",
        "        msa_status = MSA_STATUS\n",
        "\n",
        "    try:\n",
        "        # Create DataFrame from protein data\n",
        "        protein_df = pd.DataFrame(protein_data)\n",
        "        # Validate DataFrame structure using configuration\n",
        "        missing_columns = REQUIRED_OUTPUT_COLUMNS - set(protein_df.columns)\n",
        "        if missing_columns:\n",
        "            raise ValueError(f\"Missing required columns in protein data: {missing_columns}\")\n",
        "        # Create output directory if it doesn't exist\n",
        "        output_path = Path(output_dir)\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "        # Generate filename with timestamp using configuration\n",
        "        timestamp = datetime.now().strftime(CSV_TIMESTAMP_FORMAT)\n",
        "        filename = f\"Boltz2_Predictions_{protein_id}_MSA_{msa_status}_BFauber_{timestamp}.csv\"\n",
        "        filepath = output_path / filename\n",
        "        # Save to CSV\n",
        "        protein_df.to_csv(filepath, index=False)\n",
        "        print(f\"Saved results for protein {protein_id} to {filepath} ({len(protein_data)} ligands)\")\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error saving protein {protein_id} results: {e}\"\n",
        "        print(error_msg)\n",
        "        raise IOError(error_msg)\n",
        "\n",
        "\n",
        "def run_queries(df: pd.DataFrame, smiles_field: str, seq_id: str, seq: str, pic50_field: str, output_dir: str = None, msa_status: bool = None):\n",
        "    \"\"\"\n",
        "    Run Boltz2 NIM queries for all protein-ligand pairs.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing protein-ligand data\n",
        "        smiles_field: Column name for SMILES strings\n",
        "        seq_id: Protein sequence identifier\n",
        "        seq: Protein sequence string\n",
        "        pic50_field: Column name for ground truth pIC50 values\n",
        "        output_dir: Directory to save protein-specific CSV files (default: global OUTPUT_DIR)\n",
        "        msa_status: Whether to use MSA data (default: global MSA_STATUS)\n",
        "\n",
        "    Returns:\n",
        "        Tuple of result lists: (plddt_list, pic50_list, pic50_conf_list, msa_list, time_list)\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If required columns are missing\n",
        "    \"\"\"\n",
        "    # Use global constants if not provided\n",
        "    if output_dir is None:\n",
        "        output_dir = OUTPUT_DIR\n",
        "    if msa_status is None:\n",
        "        msa_status = MSA_STATUS\n",
        "\n",
        "    # Validate that required columns exist using configuration\n",
        "    required_columns = [smiles_field, pic50_field]\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
        "    # Validate DataFrame is not empty\n",
        "    if df.empty:\n",
        "        raise ValueError(\"DataFrame is empty\")\n",
        "    # Initialize result lists\n",
        "    plddt_list, pic50_list, pic50_conf_list, time_list, msa_list = [], [], [], [], []\n",
        "    # Track previous protein to avoid redundant processing\n",
        "    seq_id_prev = None\n",
        "    protein_msa = None\n",
        "    # Track current protein data for CSV saving\n",
        "    current_protein_data = []\n",
        "    current_protein_id = None\n",
        "    # Process each protein-ligand pair\n",
        "    for smile_, pic50_groundtruth in tqdm(\n",
        "        zip(df[smiles_field], df[pic50_field]),\n",
        "        desc=\"Processing protein-ligand pairs\",\n",
        "        total=len(df)\n",
        "    ):\n",
        "        # Check if we need to process a new protein\n",
        "        if seq_id_prev != seq_id:\n",
        "            # Save previous protein results to CSV if we have data\n",
        "            if current_protein_data and current_protein_id:\n",
        "                save_protein_results_to_csv(current_protein_data, current_protein_id, output_dir, msa_status)\n",
        "            # Reset for new protein\n",
        "            seq_id_prev = seq_id\n",
        "            protein_msa = None\n",
        "            current_protein_id = seq_id\n",
        "            current_protein_data = []\n",
        "            # Load MSA data if needed\n",
        "            if msa_status:\n",
        "                try:\n",
        "                    protein_msa = process_msa_file(MSA_FILE_PATH)\n",
        "                except (FileNotFoundError, IOError) as e:\n",
        "                    print(f\"Warning: {e}, skipping protein {seq_id}\")\n",
        "                    append_default_values(plddt_list, pic50_list, pic50_conf_list, msa_list, time_list, msa_status)\n",
        "                    continue\n",
        "        # Create payload for this query\n",
        "        payload = create_payload(seq, smile_, protein_msa)\n",
        "        try:\n",
        "            # Query the NIM\n",
        "            t0 = perf_counter()\n",
        "            status_code, response = boltz2_nim_query(payload)\n",
        "            t1 = perf_counter()\n",
        "            time_indv = round(t1 - t0, 3)\n",
        "            # Extract JSON from response\n",
        "            result = response.json()\n",
        "            # Extract and validate results\n",
        "            plddt_indiv, pic50_indiv, pic50_conf_indiv = validate_response(result, seq_id)\n",
        "            # Save results to lists\n",
        "            plddt_list.append(plddt_indiv)\n",
        "            pic50_list.append(pic50_indiv)\n",
        "            pic50_conf_list.append(pic50_conf_indiv)\n",
        "            msa_list.append(msa_status)\n",
        "            time_list.append(time_indv)\n",
        "            # Store current ligand data for CSV\n",
        "            current_protein_data.append({\n",
        "                'smiles': smile_,\n",
        "                'uniprot_id': seq_id,\n",
        "                'fasta_uniprot_seq': seq,\n",
        "                'pic50': pic50_groundtruth,\n",
        "                'boltz2_plddt': round(plddt_indiv, 2) if plddt_indiv is not None else None,\n",
        "                'boltz2_pic50': round(pic50_indiv[0], 2) if pic50_indiv and pic50_indiv[0] is not None else None,\n",
        "                'boltz2_pic50_conf': round(pic50_conf_indiv[0], 2) if pic50_conf_indiv and pic50_conf_indiv[0] is not None else None,\n",
        "                'boltz2_msa': msa_status,\n",
        "                'boltz2_runtime': time_indv\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to get prediction for {seq_id}: {e}\")\n",
        "            # Use a default time value if time_indv wasn't set due to early failure\n",
        "            time_val = time_indv if 'time_indv' in locals() else None\n",
        "            append_default_values(plddt_list, pic50_list, pic50_conf_list, msa_list, time_list, msa_status, time_val)\n",
        "            # Add failed ligand data to current protein data\n",
        "            current_protein_data.append({\n",
        "                'smiles': smile_,\n",
        "                'uniprot_id': seq_id,\n",
        "                'fasta_uniprot_seq': seq,\n",
        "                'pic50': pic50_groundtruth,\n",
        "                'boltz2_plddt': None,\n",
        "                'boltz2_pic50': None,\n",
        "                'boltz2_pic50_conf': None,\n",
        "                'boltz2_msa': msa_status,\n",
        "                'boltz2_runtime': time_val\n",
        "            })\n",
        "    # Save the last protein's results\n",
        "    if current_protein_data and current_protein_id:\n",
        "        save_protein_results_to_csv(current_protein_data, current_protein_id, output_dir, msa_status)\n",
        "    return plddt_list, pic50_list, pic50_conf_list, msa_list, time_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InYaT09WLiTK"
      },
      "source": [
        "  ## 2.4 Run `Boltz-2` Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ogwfQT2LiTK",
        "outputId": "f1666cbc-4ef8-4549-e92e-2d8d0545e3b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing protein-ligand pairs:   0%|          | 0/22 [00:00<?, ?it/s]<string>:0: RuntimeWarning: coroutine 'run_queries' was never awaited\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
            "Processing protein-ligand pairs:  23%|██▎       | 5/22 [00:42<01:52,  6.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to get prediction for 4wqp_1: 429: {\"status\":429,\"title\":\"Too Many Requests\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing protein-ligand pairs:  55%|█████▍    | 12/22 [01:29<00:43,  4.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to get prediction for 4wqp_1: 429: {\"status\":429,\"title\":\"Too Many Requests\"}\n",
            "Failed to get prediction for 4wqp_1: 429: {\"status\":429,\"title\":\"Too Many Requests\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing protein-ligand pairs:  68%|██████▊   | 15/22 [01:39<00:20,  2.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to get prediction for 4wqp_1: 429: {\"status\":429,\"title\":\"Too Many Requests\"}\n",
            "Failed to get prediction for 4wqp_1: 429: {\"status\":429,\"title\":\"Too Many Requests\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing protein-ligand pairs:  77%|███████▋  | 17/22 [01:39<00:07,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to get prediction for 4wqp_1: 429: {\"status\":429,\"title\":\"Too Many Requests\"}\n",
            "Failed to get prediction for 4wqp_1: 429: {\"status\":429,\"title\":\"Too Many Requests\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing protein-ligand pairs:  86%|████████▋ | 19/22 [01:39<00:02,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to get prediction for 4wqp_1: 429: {\"status\":429,\"title\":\"Too Many Requests\"}\n",
            "Failed to get prediction for 4wqp_1: 429: {\"status\":429,\"title\":\"Too Many Requests\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing protein-ligand pairs:  95%|█████████▌| 21/22 [01:39<00:00,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to get prediction for 4wqp_1: 429: {\"status\":429,\"title\":\"Too Many Requests\"}\n",
            "Failed to get prediction for 4wqp_1: 429: {\"status\":429,\"title\":\"Too Many Requests\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing protein-ligand pairs: 100%|██████████| 22/22 [01:40<00:00,  4.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to get prediction for 4wqp_1: 429: {\"status\":429,\"title\":\"Too Many Requests\"}\n",
            "Saved results for protein 4wqp_1 to /content/output/Boltz2_Predictions_4wqp_1_MSA_True_BFauber_2025_09_04.csv (22 ligands)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    loop = asyncio.get_event_loop()\n",
        "    if loop.is_running():\n",
        "        # If an event loop is already running, run the coroutine in it\n",
        "        loop.create_task(plddt_list, pic50_list, pic50_conf_list, msa_list, time_list = asyncio.run(run_queries(\n",
        "            df, \"smiles\", uniprot_id, sequence, \"actual_pIC50\",\n",
        "            OUTPUT_DIR, msa_status=MSA_STATUS)))\n",
        "except:\n",
        "      # Otherwise, create and run a new event loop\n",
        "      plddt_list, pic50_list, pic50_conf_list, msa_list, time_list = await run_queries(\n",
        "          df, \"smiles\", uniprot_id, sequence, \"actual_pIC50\",\n",
        "          OUTPUT_DIR, msa_status=MSA_STATUS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJO37QRgLiTK"
      },
      "source": [
        "  ## 2.5 Results to a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-Qad582LiTK"
      },
      "outputs": [],
      "source": [
        "# Check if all lists have the same length\n",
        "expected_length = len(df)\n",
        "actual_lengths = [len(plddt_list), len(pic50_list), len(pic50_conf_list), len(msa_list), len(time_list)]\n",
        "if not all(length == expected_length for length in actual_lengths):\n",
        "    print(f\"Warning: Result lists have different lengths. Expected: {expected_length}, Got: {actual_lengths}\")\n",
        "\n",
        "# Add Boltz2 prediction results to the original `csv_file` DataFrame\n",
        "df['boltz2_plddt'] = np.array([round(x, 2) if x is not None else None for x in plddt_list])\n",
        "df['boltz2_pic50'] = np.array([round(x[0], 2) if x and x[0] is not None else None for x in pic50_list])\n",
        "df['boltz2_pic50_conf'] = np.array([round(x[0], 2) if x and x[0] is not None else None for x in pic50_conf_list])\n",
        "df['boltz2_msa'] = np.array(msa_list)\n",
        "df['boltz2_runtime'] = np.array(time_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpdoaVCULiTK"
      },
      "source": [
        "  ## 2.6 Plot Results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ### Actual pIC50 vs Boltz2 Predicted pIC50 with regression lines and R-squared values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYA-7R-6LiTK"
      },
      "outputs": [],
      "source": [
        "# Matplotlib plot with regression lines and R-squared values for MSA cohorts\n",
        "\n",
        "BLUE = (120/255, 94/255, 240/255) # `indigo` from IBM\n",
        "ORANGE = (254/255, 97/255, 0) # from IBM\n",
        "cmap_full = [ORANGE, BLUE]\n",
        "\n",
        "# Create figure\n",
        "plt.figure(figsize=(6, 5))\n",
        "\n",
        "# Define colors for each cohort\n",
        "colors = {'False': cmap_full[1], 'True': cmap_full[0]}\n",
        "\n",
        "# Get unique MSA values\n",
        "msa_values = df['boltz2_msa'].unique()\n",
        "\n",
        "# Plot data and regression lines for each cohort\n",
        "for msa_type in msa_values:\n",
        "    # Filter data for this cohort\n",
        "    mask = df['boltz2_msa'] == msa_type\n",
        "    x_data = df[mask]['pic50'].values.reshape(-1, 1)\n",
        "    y_data = df[mask]['boltz2_pic50'].values\n",
        "\n",
        "    # Remove any NaN values\n",
        "    valid_mask = ~(np.isnan(x_data.flatten()) | np.isnan(y_data))\n",
        "    x_clean = x_data[valid_mask].reshape(-1, 1)\n",
        "    y_clean = y_data[valid_mask]\n",
        "\n",
        "    if len(x_clean) > 1:  # Need at least 2 points for regression\n",
        "        # Scatter plot\n",
        "        plt.scatter(x_clean.flatten(), y_clean,\n",
        "                   color=colors.get(msa_type),\n",
        "                   alpha=0.8,\n",
        "                   s=100,\n",
        "                   label=f'{msa_type} (n={len(x_clean)})')\n",
        "\n",
        "        # Fit linear regression\n",
        "        reg_model = LinearRegression()\n",
        "        reg_model.fit(x_clean, y_clean)\n",
        "\n",
        "        # Calculate R-squared\n",
        "        y_pred = reg_model.predict(x_clean)\n",
        "        r2 = r2_score(y_clean, y_pred)\n",
        "\n",
        "        # Create regression line\n",
        "        x_range = np.linspace(x_clean.min(), x_clean.max(), 100).reshape(-1, 1)\n",
        "        y_range = reg_model.predict(x_range)\n",
        "\n",
        "        # Plot regression line\n",
        "        plt.plot(x_range.flatten(), y_range,\n",
        "                color=colors.get(msa_type),\n",
        "                linewidth=2, linestyle='--',\n",
        "                label=f'{msa_type} R² = {r2:.2f}')\n",
        "\n",
        "# Add diagonal reference line (perfect prediction)\n",
        "min_val = min(df['pic50'].min(), df['boltz2_pic50'].min())\n",
        "max_val = max(df['pic50'].max(), df['boltz2_pic50'].max())\n",
        "plt.plot([min_val, max_val], [min_val, max_val],\n",
        "         'k--', alpha=0.8, linewidth=1, label='Perfect Prediction')\n",
        "\n",
        "# Formatting\n",
        "plt.xlabel(r\"Actual pIC$_{50}$\", fontsize=14)\n",
        "plt.ylabel(r\"Boltz-2 Predicted pIC$_{50}$\", fontsize=14)\n",
        "plt.title(r\"Boltz-2 pIC$_{50}$ vs Actual pIC$_{50}$\", fontsize=16)\n",
        "plt.legend(fontsize=12, framealpha=0.9)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tick_params(labelsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI5iIRdALiTK"
      },
      "source": [
        "  ## 2.7 Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erHqKZO5LiTL"
      },
      "outputs": [],
      "source": [
        "# Save combined results to CSV\n",
        "output_filename = f\"Boltz2_Predictions_ManyProteins_MSA_{MSA_STATUS}.csv\"\n",
        "output_filepath = Path(OUTPUT_DIR) / output_filename\n",
        "df.to_csv(output_filepath, index=False)\n",
        "\n",
        "print(f\"Saved combined results to: {output_filepath}\")\n",
        "print(f\"DataFrame shape: {df.shape}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIoLgiPxLiTL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}