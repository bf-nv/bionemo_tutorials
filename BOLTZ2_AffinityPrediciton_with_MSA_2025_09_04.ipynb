{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Boltz-2 NIM for Protein/Ligand Co-Folding and Affinity Prediction, including MSA-Search NIM for Alignments\n",
    "\n",
    "\n",
    "\n",
    " ## Demo to run in Google Colab environment\n",
    "\n",
    "\n",
    "\n",
    " [MSA-Search](https://docs.nvidia.com/nim/bionemo/msa-search/latest/overview.html) Multiple Sequence Alignment (MSA) compares a query amino acid sequence to protein databases, aligning similar sequences to identify conserved regions despite differences in length or motifs. The resulting alignments enhance structural prediction models like AlphaFold2 and OpenFold by leveraging the structural similarity of homologous sequences.\n",
    "\n",
    "\n",
    "\n",
    " [Boltz-2](https://docs.nvidia.com/nim/bionemo/boltz2/latest/index.html) Boltz-2 NIM delivers advanced biomolecular structure and binding affinity predictions for proteins, RNA, DNA, and other molecules. Built on the Boltz-2 architecture, it enables accurate modeling of complex structures and quantifies molecular interactions across diverse configurations.\n",
    "\n",
    "\n",
    "\n",
    " 29Aug2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.1 Set Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib numpy pandas seaborn sklearn tqdm httpx \"fastapi[standard]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, re, json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional\n",
    "from time import perf_counter\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from google.colab import userdata, files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Define Input File with SMILES data and Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit to match your dataset path. Ensure you use the `raw` URL path if the file is hosted on GitHub.\n",
    "#csv_file = \"https://raw.githubusercontent.com/bf-nv/bionemo_tutorials/refs/heads/main/RORc_SMILES_and_pIC50.csv\"\n",
    "\n",
    "CSV_FILE = \"RORc_SMILES_and_pIC50.csv\"\n",
    "\n",
    "OUTPUT_DIR = \"/content/output\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.2 Set Up `output` Directory and `API_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = userdata.get('API_KEY')\n",
    "\n",
    "# Ensure output directory exists, create if not present\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    shutil.rmtree(OUTPUT_DIR)\n",
    "    os.makedirs(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.3 Define `MSA-Search` Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSA_DATABASES = ['Uniref30_2302', 'colabfold_envdb_202108', 'PDB70_220313']\n",
    "\n",
    "def msa_search(sequence, API_KEY, databases=MSA_DATABASES):\n",
    "    msa_search_url = \"https://health.api.nvidia.com/v1/biology/colabfold/msa-search/predict\"\n",
    "    payload = {\n",
    "        \"sequence\": sequence,\n",
    "        \"databases\": databases,\n",
    "        \"e_value\": 0.0001,\n",
    "        \"iterations\": 1,\n",
    "        \"max_msa_sequences\": 10000,\n",
    "        \"run_structural_template_search\": False,\n",
    "        \"output_alignment_formats\": [\"a3m\"],\n",
    "    }\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"NVCF-POLL-SECONDS\": \"300\",\n",
    "    }\n",
    "    # Call MSA-Search NIM\n",
    "    response = requests.post(msa_search_url, json=payload, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def parse_sequences(input_string, n, sequence):\n",
    "    \"\"\"\n",
    "    Parse the output of alignments from the MSA-Search NIM to be used downstream\n",
    "\n",
    "    Args:\n",
    "        input_string (str): The output file of alignments in a string format\n",
    "        n (int): The amount of alignments to return from the output when parsing\n",
    "        sequence (str): The query sequence for alignment\n",
    "\n",
    "    Returns:\n",
    "        list: A list of alignment identifiers and sequences, starting with the query,\n",
    "              where the amount of sequences is given by n\n",
    "    \"\"\"\n",
    "    # Output is parsed to have a line for the sequence id and sequence itself so `n` returns correlates to n*2 lines\n",
    "    n = n * 2\n",
    "    # First, handle the `Query` block separately\n",
    "    lines = input_string.strip().split('\\n')\n",
    "    # Now process the rest of the lines\n",
    "    remaining_string = \"\\n\".join(lines[:])\n",
    "    # Regex to find blocks starting with `>` and then followed by a sequence.\n",
    "    pattern = re.compile(r'\\n>(.*?)\\n(.*?)(?=\\n>|\\Z)', re.DOTALL)\n",
    "    matches = pattern.finditer(remaining_string)\n",
    "    output_list_to_order = []\n",
    "    for match in matches:\n",
    "        # The name is the first capturing group, split by tab and take the first part\n",
    "        name_full = match.group(1).split('\\t')[0]\n",
    "        SW_score = match.group(1).split('\\t')[1]\n",
    "        # The sequence is the second capturing group\n",
    "        sequence_raw = match.group(2).strip()\n",
    "        aligned_sequence = ''.join(char for char in sequence_raw if char.isupper() or not char.isalpha())\n",
    "        # Store the aligned sequence in the list of outputs by name, sequence, Smith-Waterman score\n",
    "        output_list_to_order.append((f'>{name_full}', aligned_sequence, int(SW_score)))\n",
    "    output_lines = output_list_to_order[:n]\n",
    "    return output_lines\n",
    "\n",
    "\n",
    "def validate_a3m_format(alignments_string):\n",
    "    \"\"\"\n",
    "    Validate that the alignment string follows A3M format.\n",
    "\n",
    "    Args:\n",
    "        alignments_string (str): String containing alignments\n",
    "\n",
    "    Returns:\n",
    "        bool: True if valid A3M format, False otherwise\n",
    "    \"\"\"\n",
    "    lines = alignments_string.strip().split('\\n')\n",
    "    if len(lines) < 2:\n",
    "        return False\n",
    "\n",
    "    # Check that we have alternating header and sequence lines\n",
    "    for i, line in enumerate(lines):\n",
    "        if i % 2 == 0:  # Even indices should be headers\n",
    "            if not line.startswith('>'):\n",
    "                return False\n",
    "        else:  # Odd indices should be sequences\n",
    "            if line.startswith('>'):\n",
    "                return False\n",
    "            # Sequences should only contain valid amino acid characters and gaps\n",
    "            if not all(c in 'ACDEFGHIKLMNPQRSTVWY-' for c in line.upper()):\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def write_alignments_to_a3m(alignments_data, uniprot_id, output_dir):\n",
    "    \"\"\"\n",
    "    Write alignment data to a3M format file.\n",
    "\n",
    "    Args:\n",
    "        alignments_data: Either a list of alternating headers/sequences or a string containing alignments\n",
    "        uniprot_id (str): Uniprot ID of the protein\n",
    "        output_dir (str): Directory for the output a3M file\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the created a3M file\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    output_path = Path(output_dir) / f\"{uniprot_id}_msa_alignments.a3m\"\n",
    "\n",
    "    # Handle both list and string input formats\n",
    "    if isinstance(alignments_data, list):\n",
    "        alignments_string = '\\n'.join(alignments_data)\n",
    "    elif isinstance(alignments_data, str):\n",
    "        alignments_string = alignments_data\n",
    "    else:\n",
    "        raise ValueError(\"alignments_data must be either a list or string\")\n",
    "\n",
    "    # Validate A3M format\n",
    "    if not alignments_string.strip():\n",
    "        raise ValueError(\"Empty alignment data provided\")\n",
    "\n",
    "    # Count sequences for reporting\n",
    "    sequence_count = alignments_string.count('>')\n",
    "    if sequence_count == 0:\n",
    "        raise ValueError(\"No sequences found in alignment data\")\n",
    "\n",
    "    # Validate A3M format structure\n",
    "    if not validate_a3m_format(alignments_string):\n",
    "        print(\"Warning: Alignment data may not follow strict A3M format\")\n",
    "        print(\"Proceeding with file creation...\")\n",
    "\n",
    "    print(f\"Writing {sequence_count} sequences to A3M format: {output_path}\")\n",
    "\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            # Write the alignments\n",
    "            f.write(alignments_string)\n",
    "            # Ensure file ends with newline\n",
    "            if not alignments_string.endswith('\\n'):\n",
    "                f.write('\\n')\n",
    "\n",
    "        # Verify the file was created successfully\n",
    "        if output_path.exists():\n",
    "            file_size = output_path.stat().st_size\n",
    "            print(f\"Successfully created A3M file:\")\n",
    "            print(f\"File: {output_path}\")\n",
    "            print(f\"Size: {file_size:,} bytes\")\n",
    "            print(f\"Sequences: {sequence_count}\")\n",
    "\n",
    "            # Download the file to the user's machine\n",
    "            try:\n",
    "                files.download(str(output_path))\n",
    "                print(f\"File downloaded successfully: {output_path}\")\n",
    "            except Exception as download_error:\n",
    "                print(f\"Warning: Could not download file automatically: {download_error}\")\n",
    "                print(f\"File is available at: {output_path}\")\n",
    "\n",
    "            return str(output_path)\n",
    "        else:\n",
    "            raise IOError(f\"Failed to create file {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing A3M file: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def process_msa_alignments(msa_response_dict, sequence, uniprot_id, output_dir, databases=MSA_DATABASES, max_sequences_per_db=10000):\n",
    "    \"\"\"\n",
    "    Process MSA alignments from multiple databases and merge them into A3M format.\n",
    "\n",
    "    Args:\n",
    "        msa_response_dict (dict): MSA response data containing alignments\n",
    "        sequence (str): Query sequence for alignment\n",
    "        uniprot_id (str): Uniprot ID of the protein\n",
    "        output_dir (str): Output directory for the A3M file\n",
    "        databases (list): List of database names to process\n",
    "        max_sequences_per_db (int): Maximum number of sequences to parse per database\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the created A3M file\n",
    "    \"\"\"\n",
    "    all_parsed_dataset_output = []\n",
    "    for database in databases:\n",
    "        print(f\"Parsing results from database: {database}\")\n",
    "        # Pull string of alignments stored in json output for specific dataset\n",
    "        a3m_dict_msa_search = msa_response_dict['alignments'][database]['a3m']['alignment']\n",
    "        a3m_dict_msa_search_parsed = parse_sequences(a3m_dict_msa_search, max_sequences_per_db, sequence)\n",
    "        num_sequences_aligned = (len(a3m_dict_msa_search_parsed))\n",
    "        print(f\"Number of sequences aligned: {num_sequences_aligned}\")\n",
    "        all_parsed_dataset_output.extend(a3m_dict_msa_search_parsed)\n",
    "    # Sort all the alignments based off of the alignment score\n",
    "    all_parsed_dataset_output.sort(key=lambda x: x[2], reverse=True)\n",
    "    # Now that the alignments across all datasets are sorted, reformat each entry to name and sequence\n",
    "    sorted_parsed_output_formatted = []\n",
    "    for align_tuple in all_parsed_dataset_output:\n",
    "        sorted_parsed_output_formatted.append(align_tuple[0])\n",
    "        sorted_parsed_output_formatted.append(align_tuple[1])\n",
    "    merged_alignments_protein = [f\">query_sequence\\n{sequence}\"]\n",
    "    merged_alignments_protein.extend(sorted_parsed_output_formatted)\n",
    "    print(f\"Total merged alignments: {len(merged_alignments_protein)}\")\n",
    "    # Write merged_alignments_protein to a3M format\n",
    "    a3m_file_path = write_alignments_to_a3m(\n",
    "        merged_alignments_protein,\n",
    "        uniprot_id,\n",
    "        output_dir\n",
    "    )\n",
    "    return a3m_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.4 Run `MSA-Search` and save `A3M` alignment file output to local directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Provide Sequence Information\n",
    "\n",
    "\n",
    "\n",
    " **NOTE:** Ensure the sequence is a string and does not contain any whitespace, special characters, nor carriage returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sequence using human RORc from PDB:4wqp_A\n",
    "# http://rcsb.org/structure/4WQP\n",
    "\n",
    "# >4WQP_1|Chain A|Nuclear receptor ROR-gamma|Homo sapiens (9606)\n",
    "uniprot_id = \"4wqp_1\"\n",
    "sequence = \"MHHHHHHGENLYFQGSAPYASLTEIEHLVQSVCKSYRETCQLRLEDLLRQRSNIFSREEVTGYQRKSMWEMWERCAHHLTEAIQYVVEFAKRLSGFMELCQNDQIVLLKAGAMEVVLVRMCRAYNADNRTVFFEGKYGGMELFRALGCSELISSIFDFSHSLSALHFSEDEIALYTALVLINAHRPGLQEKRKVEQLQYNLELAFHHHLCKTHRQSILAKLPPKGKLRSLCSQHVERLQIFQHLHPIVVQAAFPPLYKELFSGNS\"\n",
    "sequences = [(uniprot_id, sequence)]\n",
    "\n",
    "# NOTE: Ensure the sequence is a string and does not contain any whitespace, special characters, nor carriage returns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_id, seq in tqdm(sequences):\n",
    "    try:\n",
    "        print(f\"\\nProcessing protein: {seq_id}\")\n",
    "        print(f\"Sequence length: {len(seq)}\")\n",
    "\n",
    "        # Call MSA-Search NIM\n",
    "        msa_response_dict = msa_search(seq, API_KEY)\n",
    "\n",
    "        # Check if the response contains the expected data\n",
    "        if 'alignments' not in msa_response_dict:\n",
    "            print(f\"Warning: No alignments found for {seq_id}\")\n",
    "            continue\n",
    "\n",
    "        # Process and create A3M file\n",
    "        a3m_file_path = process_msa_alignments(msa_response_dict, seq, seq_id, OUTPUT_DIR)\n",
    "        print(f\"Successfully processed {seq_id} -> {a3m_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {seq_id}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.5 List all created `A3M` alignment files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all created A3M files\n",
    "import glob\n",
    "a3m_files = glob.glob(f\"{OUTPUT_DIR}/*.a3m\")\n",
    "a3m_files = sorted(a3m_files)\n",
    "print(f\"Created {len(a3m_files)} A3M files:\")\n",
    "for file_path in a3m_files:\n",
    "    file_size = Path(file_path).stat().st_size\n",
    "    print(f\"  - {Path(file_path).name} ({file_size:,} bytes)\")\n",
    "\n",
    "print(f\"\\nAll A3M files are available in: {OUTPUT_DIR}\")\n",
    "print(\"Files have been automatically downloaded to your machine.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.6 If needed, trigger download of all `A3M` alignment files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in a3m_files:\n",
    "    files.download(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2.1 Set-up `Boltz-2` Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Configuration Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSA configuration\n",
    "MSA_DATA_DIR = OUTPUT_DIR\n",
    "MSA_FILE_NAME = f\"{seq_id}_msa_alignments.a3m\"\n",
    "MSA_FILE_PATH = Path(OUTPUT_DIR / MSA_FILE_NAME)\n",
    "if MSA_FILE_PATH.exists():\n",
    "    MSA_STATUS = True\n",
    "else:\n",
    "    MSA_STATUS = False\n",
    "\n",
    "# Boltz2 API configuration\n",
    "BOLTZ2_BASE_URL = \"http://localhost:8000\"\n",
    "BOLTZ2_ENDPOINT = \"/biology/mit/boltz2/predict\"\n",
    "REQUEST_TIMEOUT = 300  # 5 minutes\n",
    "\n",
    "# Boltz2 prediction parameters\n",
    "BOLTZ2_CONFIG = {\n",
    "    \"recycling_steps\": 3,\n",
    "    \"sampling_steps\": 20,\n",
    "    \"diffusion_samples\": 1,\n",
    "    \"step_scale\": 1.64,\n",
    "    \"without_potentials\": True\n",
    "}\n",
    "\n",
    "# Required output columns for protein data\n",
    "REQUIRED_OUTPUT_COLUMNS = {\n",
    "    'smiles', 'uniprot_id', 'fasta_uniprot_seq', 'pic50',\n",
    "    'boltz2_plddt', 'boltz2_pic50', 'boltz2_pic50_conf', \n",
    "    'boltz2_msa', 'boltz2_runtime'\n",
    "}\n",
    "\n",
    "# CSV timestamp format\n",
    "CSV_TIMESTAMP_FORMAT = \"%Y_%m_%d\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2.2 Use `csv_file` as Source of SMILES and pIC50 Data for Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if file exists before loading\n",
    "if not Path(CSV_FILE).exists():\n",
    "    raise FileNotFoundError(f\"CSV file not found: {CSV_FILE}\")\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(CSV_FILE, low_memory=False)\n",
    "    print(f\"Successfully loaded dataset: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "except Exception as e:\n",
    "    raise IOError(f\"Error loading CSV file {CSV_FILE}: {e}\")\n",
    "\n",
    "# Validate that the dataset has the expected structure\n",
    "if df.empty:\n",
    "    raise ValueError(\"Dataset is empty\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2.3 Define `Boltz-2` Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boltz2_nim_query(\n",
    "    input_data: Dict[str, Any], \n",
    "    base_url: str = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Query the Boltz2 NIM with input data.\n",
    "    \n",
    "    Args:\n",
    "        input_data: Dictionary containing the prediction request data\n",
    "        base_url: Base URL of the NIM service (default: from global config)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing the prediction response\n",
    "        \n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: If the HTTP request fails\n",
    "    \"\"\"\n",
    "    if base_url is None:\n",
    "        base_url = BOLTZ2_BASE_URL\n",
    "    \n",
    "    url = f\"{base_url}{BOLTZ2_ENDPOINT}\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url, \n",
    "            json=input_data, \n",
    "            headers=headers, \n",
    "            timeout=REQUEST_TIMEOUT\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.Timeout:\n",
    "        raise requests.exceptions.RequestException(f\"Request timed out after {REQUEST_TIMEOUT} seconds\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        error_msg = f\"Error querying NIM: {e}\"\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            error_msg += f\"\\nResponse status: {e.response.status_code}\"\n",
    "            if hasattr(e.response, 'text'):\n",
    "                error_msg += f\"\\nResponse text: {e.response.text}\"\n",
    "        raise requests.exceptions.RequestException(error_msg)\n",
    "\n",
    "\n",
    "def create_payload(seq: str, smile_: str, msa_content: str = None) -> Dict[str, Any]:\n",
    "    \"\"\"Create the payload for Boltz2 NIM query.\n",
    "    \n",
    "    Args:\n",
    "        seq (str): Protein sequence\n",
    "        smile_: SMILES string for the ligand\n",
    "        msa_content (str, optional): MSA alignment content as string\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Any]: Payload for Boltz2 NIM query\n",
    "    \"\"\"\n",
    "    polymer_data = {\n",
    "        \"id\": \"A\",\n",
    "        \"molecule_type\": \"protein\",\n",
    "        \"sequence\": seq,\n",
    "    }\n",
    "    \n",
    "    # Add MSA data if content is provided\n",
    "    if msa_content:\n",
    "        polymer_data[\"msa\"] = {\n",
    "            \"uniref90\": {\n",
    "                \"a3m\": {\n",
    "                    \"alignment\": msa_content,\n",
    "                    \"format\": \"a3m\"\n",
    "                }\n",
    "            }\n",
    "        }    \n",
    "    # Construct complete payload using configuration constants\n",
    "    payload = {\n",
    "        \"polymers\": [polymer_data],\n",
    "        \"ligands\": [{\n",
    "            \"smiles\": smile_,\n",
    "            \"id\": \"L1\",\n",
    "            \"predict_affinity\": True\n",
    "        }],\n",
    "        **BOLTZ2_CONFIG\n",
    "    }\n",
    "    return payload\n",
    "\n",
    "\n",
    "def append_default_values(plddt_list, pic50_list, pic50_conf_list, msa_list, time_list, pic50_groundtruth, msa_status, time_val=None, csv_file=None):\n",
    "    \"\"\"Append default values to all result lists while preserving ground truth data.\n",
    "    \n",
    "    Note: This function is used to maintain the correct length of Boltz2 prediction result lists.\n",
    "    The pic50_groundtruth parameter is included for consistency but not stored in these lists\n",
    "    since ground truth values are preserved separately in the original DataFrame and protein CSV files.\n",
    "    \n",
    "    Args:\n",
    "        plddt_list: List to append pLDDT values to\n",
    "        pic50_list: List to append pIC50 values to  \n",
    "        pic50_conf_list: List to append pIC50 confidence values to\n",
    "        msa_list: List to append MSA status to\n",
    "        time_list: List to append timing values to\n",
    "        pic50_groundtruth: Ground truth pIC50 value (for reference)\n",
    "        msa_status: Whether MSA was used\n",
    "        time_val: Runtime value to append (optional)\n",
    "        csv_file: Path to CSV file for reference (optional, uses global CSV_FILE if not provided)\n",
    "    \"\"\"\n",
    "    # Use global CSV_FILE if csv_file parameter not provided\n",
    "    if csv_file is None:\n",
    "        csv_file = CSV_FILE\n",
    "    \n",
    "    # Append default/null values to maintain list consistency\n",
    "    plddt_list.append(None)\n",
    "    pic50_list.append(None)\n",
    "    pic50_conf_list.append(None)\n",
    "    msa_list.append(msa_status)\n",
    "    time_list.append(time_val)\n",
    "    \n",
    "    # Log the CSV file being referenced for debugging/tracking\n",
    "    if csv_file:\n",
    "        print(f\"Default values appended for failed prediction (CSV source: {csv_file})\")\n",
    "\n",
    "\n",
    "def process_msa_file(msa_file_path: Path = None) -> str:\n",
    "    \"\"\"\n",
    "    Process MSA file and return its content.\n",
    "    \n",
    "    Args:\n",
    "        msa_file_path: Path to MSA file (default: global MSA_FILE_PATH)\n",
    "        \n",
    "    Returns:\n",
    "        MSA alignment data as string\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If MSA file doesn't exist\n",
    "        IOError: If there's an error reading the file\n",
    "    \"\"\"\n",
    "    if msa_file_path is None:\n",
    "        msa_file_path = MSA_FILE_PATH\n",
    "    \n",
    "    if not msa_file_path.exists():\n",
    "        raise FileNotFoundError(f\"MSA file not found: {msa_file_path}\")\n",
    "    try:\n",
    "        with open(msa_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read().strip()\n",
    "            if not content:\n",
    "                raise IOError(f\"MSA file is empty: {msa_file_path}\")\n",
    "            return content\n",
    "    except UnicodeDecodeError as e:\n",
    "        raise IOError(f\"Error decoding MSA file {msa_file_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error reading MSA file {msa_file_path}: {e}\")\n",
    "\n",
    "\n",
    "def validate_response(result: Dict[str, Any], seq_id: str) -> tuple[Optional[float], list, list]:\n",
    "    \"\"\"\n",
    "    Validate API response and extract required data.\n",
    "    \n",
    "    Args:\n",
    "        result: API response dictionary\n",
    "        seq_id: Protein sequence identifier for error reporting\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (plddt_score, pic50_values, pic50_confidence_values)\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If required data is missing from response\n",
    "    \"\"\"\n",
    "    # Validate affinities data\n",
    "    if 'affinities' not in result:\n",
    "        raise ValueError(f\"Missing 'affinities' key in response for {seq_id}\")\n",
    "    if 'L1' not in result['affinities']:\n",
    "        raise ValueError(f\"Missing 'L1' ligand data in affinities for {seq_id}\")\n",
    "    # Extract pLDDT scores\n",
    "    if 'complex_plddt_scores' not in result or not result['complex_plddt_scores']:\n",
    "        print(f\"Warning: Missing or empty pLDDT scores for {seq_id}\")\n",
    "        plddt_indiv = None\n",
    "    else:\n",
    "        try:\n",
    "            plddt_indiv = float(np.mean(result['complex_plddt_scores']))\n",
    "        except (TypeError, ValueError) as e:\n",
    "            print(f\"Warning: Error calculating pLDDT mean for {seq_id}: {e}\")\n",
    "            plddt_indiv = None\n",
    "    # Extract affinity data\n",
    "    try:\n",
    "        pic50_indiv = result['affinities']['L1']['affinity_pic50']\n",
    "        pic50_conf_indiv = result['affinities']['L1']['affinity_probability_binary']\n",
    "        # Validate that we got lists\n",
    "        if not isinstance(pic50_indiv, list) or not isinstance(pic50_conf_indiv, list):\n",
    "            raise ValueError(f\"Expected lists for affinity data, got {type(pic50_indiv)} and {type(pic50_conf_indiv)}\")\n",
    "    except KeyError as e:\n",
    "        raise ValueError(f\"Missing required affinity data key {e} for {seq_id}\")\n",
    "    return plddt_indiv, pic50_indiv, pic50_conf_indiv\n",
    "\n",
    "\n",
    "def save_protein_results_to_csv(protein_data: list, protein_id: str, output_dir: str = None, msa_status: bool = None):\n",
    "    \"\"\"\n",
    "    Save results for a single protein to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        protein_data: List of dictionaries containing ligand data\n",
    "        protein_id: Identifier for the protein\n",
    "        output_dir: Directory to save the CSV file (default: global OUTPUT_DIR)\n",
    "        msa_status: Whether MSA was used for predictions (default: global MSA_STATUS)\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If protein_data is empty or invalid\n",
    "        IOError: If there's an error writing the file\n",
    "    \"\"\"\n",
    "    if not protein_data:\n",
    "        return\n",
    "    if not isinstance(protein_data, list) or not all(isinstance(item, dict) for item in protein_data):\n",
    "        raise ValueError(\"protein_data must be a list of dictionaries\")\n",
    "    \n",
    "    # Use global constants if not provided\n",
    "    if output_dir is None:\n",
    "        output_dir = OUTPUT_DIR\n",
    "    if msa_status is None:\n",
    "        msa_status = MSA_STATUS\n",
    "    \n",
    "    try:\n",
    "        # Create DataFrame from protein data\n",
    "        protein_df = pd.DataFrame(protein_data)\n",
    "        # Validate DataFrame structure using configuration\n",
    "        missing_columns = REQUIRED_OUTPUT_COLUMNS - set(protein_df.columns)\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns in protein data: {missing_columns}\")\n",
    "        # Create output directory if it doesn't exist\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        # Generate filename with timestamp using configuration\n",
    "        timestamp = datetime.now().strftime(CSV_TIMESTAMP_FORMAT)\n",
    "        filename = f\"Boltz2_Predictions_{protein_id}_MSA_{msa_status}_BFauber_{timestamp}.csv\"\n",
    "        filepath = output_path / filename\n",
    "        # Save to CSV\n",
    "        protein_df.to_csv(filepath, index=False)\n",
    "        print(f\"Saved results for protein {protein_id} to {filepath} ({len(protein_data)} ligands)\")\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error saving protein {protein_id} results: {e}\"\n",
    "        print(error_msg)\n",
    "        raise IOError(error_msg)\n",
    "    \n",
    "\n",
    "def run_queries(df: pd.DataFrame, smiles_field: str, seq_id_field: str, seq_field: str, pic50_field: str, output_dir: str = None, msa_status: bool = None):\n",
    "    \"\"\"\n",
    "    Run Boltz2 NIM queries for all protein-ligand pairs.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing protein-ligand data\n",
    "        smiles_field: Column name for SMILES strings\n",
    "        seq_id_field: Column name for sequence IDs\n",
    "        seq_field: Column name for protein sequences\n",
    "        pic50_field: Column name for ground truth pIC50 values\n",
    "        output_dir: Directory to save protein-specific CSV files (default: global OUTPUT_DIR)\n",
    "        msa_status: Whether to use MSA data (default: global MSA_STATUS)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of result lists: (plddt_list, pic50_list, pic50_conf_list, msa_list, time_list)\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If required columns are missing\n",
    "    \"\"\"\n",
    "    # Use global constants if not provided\n",
    "    if output_dir is None:\n",
    "        output_dir = OUTPUT_DIR\n",
    "    if msa_status is None:\n",
    "        msa_status = MSA_STATUS\n",
    "    \n",
    "    # Validate that required columns exist using configuration\n",
    "    required_columns = [smiles_field, seq_id_field, seq_field, pic50_field]\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    # Validate DataFrame is not empty\n",
    "    if df.empty:\n",
    "        raise ValueError(\"DataFrame is empty\")\n",
    "    # Initialize result lists\n",
    "    plddt_list, pic50_list, pic50_conf_list, time_list, msa_list = [], [], [], [], []\n",
    "    # Track previous protein to avoid redundant processing\n",
    "    seq_id_prev = None\n",
    "    protein_msa = None\n",
    "    # Track current protein data for CSV saving\n",
    "    current_protein_data = []\n",
    "    current_protein_id = None\n",
    "    # Process each protein-ligand pair\n",
    "    for smile_, seq_id, seq, pic50_groundtruth in tqdm(\n",
    "        zip(df[smiles_field], df[seq_id_field], df[seq_field], df[pic50_field]),\n",
    "        desc=\"Processing protein-ligand pairs\",\n",
    "        total=len(df)\n",
    "    ):\n",
    "        # Check if we need to process a new protein\n",
    "        if seq_id_prev != seq_id:\n",
    "            # Save previous protein results to CSV if we have data\n",
    "            if current_protein_data and current_protein_id:\n",
    "                save_protein_results_to_csv(current_protein_data, current_protein_id, output_dir, msa_status)\n",
    "            # Reset for new protein\n",
    "            seq_id_prev = seq_id\n",
    "            protein_msa = None\n",
    "            current_protein_id = seq_id\n",
    "            current_protein_data = []\n",
    "            # Load MSA data if needed\n",
    "            if msa_status:\n",
    "                try:\n",
    "                    protein_msa = process_msa_file(MSA_FILE_PATH)\n",
    "                except (FileNotFoundError, IOError) as e:\n",
    "                    print(f\"Warning: {e}, skipping protein {seq_id}\")\n",
    "                    append_default_values(plddt_list, pic50_list, pic50_conf_list, msa_list, time_list, pic50_groundtruth, msa_status, csv_file=CSV_FILE)\n",
    "                    continue\n",
    "        # Create payload for this query\n",
    "        payload = create_payload(seq, smile_, protein_msa)\n",
    "        try:\n",
    "            # Query the NIM\n",
    "            t0 = perf_counter()\n",
    "            result = boltz2_nim_query(payload)\n",
    "            t1 = perf_counter()\n",
    "            time_indv = round(t1 - t0, 3)\n",
    "            # Extract and validate results\n",
    "            plddt_indiv, pic50_indiv, pic50_conf_indiv = validate_response(result, seq_id)\n",
    "            # Save results to lists\n",
    "            plddt_list.append(plddt_indiv)\n",
    "            pic50_list.append(pic50_indiv)\n",
    "            pic50_conf_list.append(pic50_conf_indiv)\n",
    "            msa_list.append(msa_status)\n",
    "            time_list.append(time_indv)\n",
    "            # Store current ligand data for CSV\n",
    "            current_protein_data.append({\n",
    "                'smiles': smile_,\n",
    "                'uniprot_id': seq_id,\n",
    "                'fasta_uniprot_seq': seq,\n",
    "                'pic50': pic50_groundtruth,\n",
    "                'boltz2_plddt': round(plddt_indiv, 2) if plddt_indiv is not None else None,\n",
    "                'boltz2_pic50': round(pic50_indiv[0], 2) if pic50_indiv and pic50_indiv[0] is not None else None,\n",
    "                'boltz2_pic50_conf': round(pic50_conf_indiv[0], 2) if pic50_conf_indiv and pic50_conf_indiv[0] is not None else None,\n",
    "                'boltz2_msa': msa_status,\n",
    "                'boltz2_runtime': time_indv\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get prediction for {seq_id}: {e}\")\n",
    "            append_default_values(plddt_list, pic50_list, pic50_conf_list, msa_list, time_list, pic50_groundtruth, msa_status, time_indv if 'time_indv' in locals() else None, csv_file=CSV_FILE)\n",
    "            # Add failed ligand data to current protein data\n",
    "            current_protein_data.append({\n",
    "                'smiles': smile_,\n",
    "                'uniprot_id': seq_id,\n",
    "                'fasta_uniprot_seq': seq,\n",
    "                'pic50': pic50_groundtruth,\n",
    "                'boltz2_plddt': None,\n",
    "                'boltz2_pic50': None,\n",
    "                'boltz2_pic50_conf': None,\n",
    "                'boltz2_msa': msa_status,\n",
    "                'boltz2_runtime': time_indv if 'time_indv' in locals() else None\n",
    "            })\n",
    "    # Save the last protein's results\n",
    "    if current_protein_data and current_protein_id:\n",
    "        save_protein_results_to_csv(current_protein_data, current_protein_id, output_dir, msa_status)\n",
    "    return plddt_list, pic50_list, pic50_conf_list, msa_list, time_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2.4 Run `Boltz-2` Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plddt_list, pic50_list, pic50_conf_list, msa_list, time_list = run_queries(\n",
    "    df, \"smiles\", \"uniprot_id\", \"fasta_uniprot_seq\", \"pic50\", \n",
    "    OUTPUT_DIR, msa_status=MSA_STATUS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2.5 Results to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all lists have the same length\n",
    "expected_length = len(df)\n",
    "actual_lengths = [len(plddt_list), len(pic50_list), len(pic50_conf_list), len(msa_list), len(time_list)]\n",
    "if not all(length == expected_length for length in actual_lengths):\n",
    "    print(f\"Warning: Result lists have different lengths. Expected: {expected_length}, Got: {actual_lengths}\")\n",
    "\n",
    "# Add Boltz2 prediction results to the original `csv_file` DataFrame\n",
    "df['boltz2_plddt'] = np.array([round(x, 2) if x is not None else None for x in plddt_list])\n",
    "df['boltz2_pic50'] = np.array([round(x[0], 2) if x and x[0] is not None else None for x in pic50_list])\n",
    "df['boltz2_pic50_conf'] = np.array([round(x[0], 2) if x and x[0] is not None else None for x in pic50_conf_list])\n",
    "df['boltz2_msa'] = np.array(msa_list)\n",
    "df['boltz2_runtime'] = np.array(time_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2.6 Plot Results\n",
    "\n",
    "\n",
    "\n",
    " ### Actual pIC50 vs Boltz2 Predicted pIC50 with regression lines and R-squared values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib plot with regression lines and R-squared values for MSA cohorts\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "BLUE = (120/255, 94/255, 240/255) # `indigo` from IBM\n",
    "ORANGE = (254/255, 97/255, 0) # from IBM\n",
    "RED = (220/255, 38/255, 127/255) # `magenta` from IBM\n",
    "cmap_full = [ORANGE, BLUE, RED]\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "# Define colors for each cohort\n",
    "colors = {'False': cmap_full[1], 'True': cmap_full[0]}\n",
    "\n",
    "# Get unique MSA values\n",
    "msa_values = df['boltz2_msa'].unique()\n",
    "\n",
    "# Plot data and regression lines for each cohort\n",
    "for msa_type in msa_values:\n",
    "    # Filter data for this cohort\n",
    "    mask = df['boltz2_msa'] == msa_type\n",
    "    x_data = df[mask]['pic50'].values.reshape(-1, 1)\n",
    "    y_data = df[mask]['boltz2_pic50'].values\n",
    "    \n",
    "    # Remove any NaN values\n",
    "    valid_mask = ~(np.isnan(x_data.flatten()) | np.isnan(y_data))\n",
    "    x_clean = x_data[valid_mask].reshape(-1, 1)\n",
    "    y_clean = y_data[valid_mask]\n",
    "    \n",
    "    if len(x_clean) > 1:  # Need at least 2 points for regression\n",
    "        # Scatter plot\n",
    "        plt.scatter(x_clean.flatten(), y_clean, \n",
    "                   color=colors.get(msa_type, \n",
    "                                    #'#333333'\n",
    "                                    ), \n",
    "                   alpha=0.8, \n",
    "                   s=100, \n",
    "                   label=f'{msa_type} (n={len(x_clean)})')\n",
    "        \n",
    "        # Fit linear regression\n",
    "        reg_model = LinearRegression()\n",
    "        reg_model.fit(x_clean, y_clean)\n",
    "        \n",
    "        # Calculate R-squared\n",
    "        y_pred = reg_model.predict(x_clean)\n",
    "        r2 = r2_score(y_clean, y_pred)\n",
    "        \n",
    "        # Create regression line\n",
    "        x_range = np.linspace(x_clean.min(), x_clean.max(), 100).reshape(-1, 1)\n",
    "        y_range = reg_model.predict(x_range)\n",
    "        \n",
    "        # Plot regression line\n",
    "        plt.plot(x_range.flatten(), y_range, \n",
    "                color=colors.get(msa_type, \n",
    "                                 #'#333333'\n",
    "                                 ), \n",
    "                linewidth=2, linestyle='--',\n",
    "                label=f'{msa_type} R² = {r2:.2f}')\n",
    "\n",
    "# Add diagonal reference line (perfect prediction)\n",
    "min_val = min(df['pic50'].min(), df['boltz2_pic50'].min())\n",
    "max_val = max(df['pic50'].max(), df['boltz2_pic50'].max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], \n",
    "         'k--', alpha=0.8, linewidth=1, label='Perfect Prediction')\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel(r\"Actual pIC$_{50}$\", fontsize=14)\n",
    "plt.ylabel(r\"Boltz-2 Predicted pIC$_{50}$\", fontsize=14)\n",
    "plt.title(r\"Boltz-2 pIC$_{50}$ vs Actual pIC$_{50}$\", fontsize=16)\n",
    "plt.legend(fontsize=12, framealpha=0.9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2.7 Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined results to CSV\n",
    "output_filename = f\"Boltz2_Predictions_ManyProteins_MSA_{MSA_STATUS}.csv\"\n",
    "output_filepath = Path(OUTPUT_DIR) / output_filename\n",
    "df.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"Saved combined results to: {output_filepath}\")\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
